@online{dropboxRust,
  title={The Epic Story of Dropbox's Exodus From the Amazon Cloud Empire},
  author={Cade Metz},
  medium={Электронный ресурс},
  url = {https://www.wired.com/2016/03/epic-story-dropboxs-exodus-amazon-cloud-empire/},
  urldate = {26.11.2022},
  language = {english}
}

@inproceedings{10.1145/2628136.2628161,
author = {Vazou, Niki and Seidel, Eric L. and Jhala, Ranjit and Vytiniotis, Dimitrios and Peyton-Jones, Simon},
title = {Refinement Types for Haskell},
year = {2014},
isbn = {9781450328739},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628136.2628161},
doi = {10.1145/2628136.2628161},
abstract = {SMT-based checking of refinement types for call-by-value languages is a well-studied subject. Unfortunately, the classical translation of refinement types to verification conditions is unsound under lazy evaluation. When checking an expression, such systems implicitly assume that all the free variables in the expression are bound to values. This property is trivially guaranteed by eager, but does not hold under lazy, evaluation. Thus, to be sound and precise, a refinement type system for Haskell and the corresponding verification conditions must take into account which subset of binders actually reduces to values. We present a stratified type system that labels binders as potentially diverging or not, and that (circularly) uses refinement types to verify the labeling. We have implemented our system in LIQUIDHASKELL and present an experimental evaluation of our approach on more than 10,000 lines of widely used Haskell libraries. We show that LIQUIDHASKELL is able to prove 96% of all recursive functions terminating, while requiring a modest 1.7 lines of termination-annotations per 100 lines of code.},
booktitle = {Proceedings of the 19th ACM SIGPLAN International Conference on Functional Programming},
pages = {269–282},
numpages = {14},
location = {Gothenburg, Sweden},
series = {ICFP '14}
}
@InProceedings{10.1007/978-3-642-17511-4_20,
author="Leino, K. Rustan M.",
editor="Clarke, Edmund M.
and Voronkov, Andrei",
title="Dafny: An Automatic Program Verifier for Functional Correctness",
booktitle="Logic for Programming, Artificial Intelligence, and Reasoning",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="348--370",
abstract="Traditionally, the full verification of a program's functional correctness has been obtained with pen and paper or with interactive proof assistants, whereas only reduced verification tasks, such as extended static checking, have enjoyed the automation offered by satisfiability-modulo-theories (SMT) solvers. More recently, powerful SMT solvers and well-designed program verifiers are starting to break that tradition, thus reducing the effort involved in doing full verification.",
isbn="978-3-642-17511-4"
}
@InProceedings{10.1007/978-3-662-49122-5_2,
author="M{\"u}ller, Peter
and Schwerhoff, Malte
and Summers, Alexander J.",
editor="Jobstmann, Barbara
and Leino, K. Rustan M.",
title="Viper: A Verification Infrastructure for Permission-Based Reasoning",
booktitle="Verification, Model Checking, and Abstract Interpretation",
year="2016",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="41--62",
abstract="The automation of verification techniques based on first-order logic specifications has benefitted greatly from verification infrastructures such as Boogie and Why. These offer an intermediate language that can express diverse language features and verification techniques, as well as back-end tools: in particular, verification condition generators.",
isbn="978-3-662-49122-5"
}

@article{MAKOWSKY1987266,
title = {Why horn formulas matter in computer science: Initial structures and generic examples},
journal = {Journal of Computer and System Sciences},
volume = {34},
number = {2},
pages = {266-292},
year = {1987},
issn = {0022-0000},
doi = {https://doi.org/10.1016/0022-0000(87)90027-4},
url = {https://www.sciencedirect.com/science/article/pii/0022000087900274},
author = {J.A. Makowsky},
abstract = {We introduce the notion of generic examples as a unifying principle for various phenomena in computer science such as initial structures in the area of abstract data types, and Armstrong relations in the area of data bases. Generic examples are also useful in defining the semantics of logic programming, in the formal theory of program testing and in complexity theory. We characterize initial structures in terms of their genericity properties and give a syntactic characterization of first-order theories admitting initial .structures. The latter can be used to explain why Horn formulas have gained such a predominant role in various areas of computer science.}
}

@InProceedings{claessen2015tip,
author="Claessen, Koen
and Johansson, Moa
and Rosen, Dan
and Smallbone, Nicholas",
editor="Kerber, Manfred
and Carette, Jacques
and Kaliszyk, Cezary
and Rabe, Florian
and Sorge, Volker",
title="TIP: Tons of Inductive Problems",
booktitle="Intelligent Computer Mathematics",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="333--337",
abstract="This paper describes our collection of benchmarks for inductive theorem provers. The recent spur of interest in automated inductive theorem proving has increased the demands for evaluation and comparison between systems. We expect the benchmark suite to continually grow as more problems are submitted by the community. New challenge problems will promote further development of provers which will greatly benefit both developers and users of inductive theorem provers.",
isbn="978-3-319-20615-8"
}

@InProceedings{stump2014starexec,
author="Stump, Aaron
and Sutcliffe, Geoff
and Tinelli, Cesare",
editor="Demri, St{\'e}phane
and Kapur, Deepak
and Weidenbach, Christoph",
title="StarExec: A Cross-Community Infrastructure for Logic Solving",
booktitle="Automated Reasoning",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="367--373",
abstract="We introduce StarExec, a public web-based service built to facilitate the experimental evaluation of logic solvers, broadly understood as automated tools based on formal reasoning. Examples of such tools include theorem provers, SAT and SMT solvers, constraint solvers, model checkers, and software verifiers. The service, running on a compute cluster with 380 processors and 23 terabytes of disk space, is designed to provide a single piece of storage and computing infrastructure to logic solving communities and their members. It aims at reducing duplication of effort and resources as well as enabling individual researchers or groups with no access to comparable infrastructure. StarExec allows community organizers to store, manage and make available benchmark libraries; competition organizers to run logic solver competitions; and community members to do comparative evaluations of logic solvers on public or private benchmark problems.",
isbn="978-3-319-08587-6"
}

@article{10.1145/3498722,
author = {K, Hari Govind V and Shoham, Sharon and Gurfinkel, Arie},
title = {Solving Constrained Horn Clauses modulo Algebraic Data Types and Recursive Functions},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {POPL},
url = {https://doi.org/10.1145/3498722},
doi = {10.1145/3498722},
abstract = {This work addresses the problem of verifying imperative programs that manipulate data structures, e.g., Rust programs. Data structures are usually modeled by Algebraic Data Types (ADTs) in verification conditions. Inductive invariants of such programs often require recursively defined functions (RDFs) to represent abstractions of data structures. From the logic perspective, this reduces to solving Constrained Horn Clauses (CHCs) modulo both ADT and RDF. The underlying logic with RDFs is undecidable. Thus, even verifying a candidate inductive invariant is undecidable. Similarly, IC3-based algorithms for solving CHCs lose their progress guarantee: they may not find counterexamples when the program is unsafe. We propose a novel IC3-inspired algorithm Racer for solving CHCs modulo ADT and RDF (i.e., automatically synthesizing inductive invariants, as opposed to only verifying them as is done in deductive verification). Racer ensures progress despite the undecidability of the underlying theory, and is guaranteed to terminate with a counterexample for unsafe programs. It works with a general class of RDFs over ADTs called catamorphisms. The key idea is to represent catamorphisms as both CHCs, via relationification, and RDFs, using novel abstractions. Encoding catamorphisms as CHCs allows learning inductive properties of catamorphisms, as well as preserving unsatisfiabilty of the original CHCs despite the use of RDF abstractions, whereas encoding catamorphisms as RDFs allows unfolding the recursive definition, and relying on it in solutions. Abstractions ensure that the underlying theory remains decidable. We implement our approach in Z3 and show that it works well in practice.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {60},
numpages = {29},
keywords = {Model Checking, Formal verification, Recursive Functions, Algebraic Data Types}
}

@article{de_angelis_proietti_fioravanti_pettorossi_2022,
  title={Verifying Catamorphism-Based Contracts using Constrained Horn Clauses},
  volume={22},
  DOI={10.1017/S1471068422000175},
  number={4},
  journal={Theory and Practice of Logic Programming}, publisher={Cambridge University Press},
  author={de Angelis, Emanuele and Proietti, Maurizio and Fioravanti, Fabio and Pettorossi, Alberto},
  year={2022},
  pages={555–572}
}

@article{10.1145/3462205,
author = {Matsushita, Yusuke and Tsukada, Takeshi and Kobayashi, Naoki},
title = {RustHorn: CHC-Based Verification for Rust Programs},
year = {2021},
issue_date = {December 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0164-0925},
url = {https://doi.org/10.1145/3462205},
doi = {10.1145/3462205},
abstract = {Reduction to satisfiability of constrained Horn clauses (CHCs) is a widely studied approach to automated program verification. Current CHC-based methods, however, do not work very well for pointer-manipulating programs, especially those with dynamic memory allocation. This article presents a novel reduction of pointer-manipulating Rust programs into CHCs, which clears away pointers and memory states by leveraging Rust’s guarantees on permission. We formalize our reduction for a simplified core of Rust and prove its soundness and completeness. We have implemented a prototype verifier for a subset of Rust and confirmed the effectiveness of our method.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {oct},
articleno = {15},
numpages = {54},
keywords = {pointer, Rust, permission, automated verification, ownership, CHC}
}

@InProceedings{10.1007/978-3-031-13185-1_16,
author="Alt, Leonardo
and Blicha, Martin
and Hyv{\"a}rinen, Antti E. J.
and Sharygina, Natasha",
editor="Shoham, Sharon
and Vizel, Yakir",
title="SolCMC: Solidity Compiler's Model Checker",
booktitle="Computer Aided Verification",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="325--338",
abstract="Formally verifying smart contracts is important due to their immutable nature, usual open source licenses, and high financial incentives for exploits. Since 2019 the Ethereum Foundation's Solidity compiler ships with a model checker. The checker, called SolCMC, has two different reasoning engines and tracks closely the development of the Solidity language. We describe SolCMC's architecture and use from the perspective of developers of both smart contracts and tools for software verification, and show how to analyze nontrivial properties of real life contracts in a fully automated manner.",
isbn="978-3-031-13185-1"
}

@article{barrett2007abstract,
  title={An abstract decision procedure for a theory of inductive data types},
  author={Barrett, Clark and Shikanian, Igor and Tinelli, Cesare},
  journal={Journal on Satisfiability, Boolean Modeling and Computation},
  volume={3},
  number={1-2},
  pages={21--46},
  year={2007},
  publisher={IOS Press}
}

@article{pham2016reasoning,
  title={Reasoning about algebraic data types with abstractions},
  author={Pham, Tuan-Hung and Gacek, Andrew and Whalen, Michael W},
  journal={Journal of Automated Reasoning},
  volume={57},
  number={4},
  pages={281--318},
  year={2016},
  publisher={Springer}
}

@inproceedings{kapur2006interpolation,
author = {Kapur, Deepak and Majumdar, Rupak and Zarba, Calogero G.},
title = {Interpolation for Data Structures},
year = {2006},
isbn = {1595934685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1181775.1181789},
doi = {10.1145/1181775.1181789},
abstract = {Interpolation based automatic abstraction is a powerful and robust technique for the automated analysis of hardware and software systems. Its use has however been limited to control-dominated applications because of a lack of algorithms for computing interpolants for data structures used in software programs. We present efficient procedures to construct interpolants for the theories of arrays, sets, and multisets using the reduction approach for obtaining decision procedures for complex data structures. The approach taken is that of reducing the theories of such data structures to the theories of equality and linear arithmetic for which efficient interpolating decision procedures exist. This enables interpolation based techniques to be applied to proving properties of programs that manipulate these data structures.},
booktitle = {Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {105–116},
numpages = {12},
keywords = {interpolation, CEGAR, data structure verification},
location = {Portland, Oregon, USA},
series = {SIGSOFT '06/FSE-14}
}

@inproceedings{suter2010decision,
author = {Suter, Philippe and Dotta, Mirco and Kuncak, Viktor},
title = {Decision Procedures for Algebraic Data Types with Abstractions},
year = {2010},
isbn = {9781605584799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1706299.1706325},
doi = {10.1145/1706299.1706325},
abstract = {We describe a family of decision procedures that extend the decision procedure for quantifier-free constraints on recursive algebraic data types (term algebras) to support recursive abstraction functions. Our abstraction functions are catamorphisms (term algebra homomorphisms) mapping algebraic data type values into values in other decidable theories (e.g. sets, multisets, lists, integers, booleans). Each instance of our decision procedure family is sound; we identify a widely applicable many-to-one condition on abstraction functions that implies the completeness. Complete instances of our decision procedure include the following correctness statements: 1) a functional data structure implementation satisfies a recursively specified invariant, 2) such data structure conforms to a contract given in terms of sets, multisets, lists, sizes, or heights, 3) a transformation of a formula (or lambda term) abstract syntax tree changes the set of free variables in the specified way.},
booktitle = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {199–210},
numpages = {12},
keywords = {decision procedure, catamorphism, data structure, algebraic data type},
location = {Madrid, Spain},
series = {POPL '10}
}

@InProceedings{reynolds2017decision,
author="Reynolds, Andrew
and Blanchette, Jasmin Christian",
editor="Felty, Amy P.
and Middeldorp, Aart",
title="A Decision Procedure for (Co)datatypes in SMT Solvers",
booktitle="Automated Deduction - CADE-25",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="197--213",
abstract="We present a decision procedure that combines reasoning about datatypes and codatatypes. The dual of the acyclicity rule for datatypes is a uniqueness rule that identifies observationally equal codatatype values, including cyclic values. The procedure decides universal problems and is composable via the Nelson--Oppen method. It has been implemented in CVC4, a state-of-the-art SMT solver. An evaluation based on problems generated from theories developed with Isabelle demonstrates the potential of the procedure.",
isbn="978-3-319-21401-6"
}

@INPROCEEDINGS{8603013,
  author={Hojjat, Hossein and Rümmer, Philipp},
  booktitle={2018 Formal Methods in Computer Aided Design (FMCAD)}, 
  title={The ELDARICA Horn Solver}, 
  year={2018},
  volume={},
  number={},
  pages={1-7},
  doi={10.23919/FMCAD.2018.8603013}}

@article{pettorossi_proietti_2022, title={Analysis and Transformation of Constrained Horn Clauses for Program Verification}, volume={22}, DOI={10.1017/S1471068421000211}, number={6}, journal={Theory and Practice of Logic Programming}, publisher={Cambridge University Press}, author={De Angelis, Emanuele and Fioravanti, Fabio and Gallagher, John P and Hermenegildo, Manuel V and Pettorossi, Alberto and Proietti, Maurizio}, year={2022}, pages={974–1042}}

@InProceedings{10.1007/978-3-030-51074-9_6,
author="De Angelis, Emanuele
and Fioravanti, Fabio
and Pettorossi, Alberto
and Proietti, Maurizio",
editor="Peltier, Nicolas
and Sofronie-Stokkermans, Viorica",
title="Removing Algebraic Data Types from Constrained Horn Clauses Using Difference Predicates",
booktitle="Automated Reasoning",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="83--102",
abstract="We address the problem of proving the satisfiability of Constrained Horn Clauses (CHCs) with Algebraic Data Types (ADTs), such as lists and trees. We propose a new technique for transforming CHCs with ADTs into CHCs where predicates are defined over basic types, such as integers and booleans, only. Thus, our technique avoids the explicit use of inductive proof rules during satisfiability proofs. The main extension over previous techniques for ADT removal is a new transformation rule, called differential replacement, which allows us to introduce auxiliary predicates corresponding to the lemmas used when making inductive proofs. We present an algorithm that applies the new rule, together with the traditional folding/unfolding rules, for the automatic removal of ADTs. We prove that if the set of the transformed clauses is satisfiable, then so is the set of the original clauses. By an experimental evaluation, we show that the use of the new rule significantly improves the effectiveness of ADT removal, and that our approach is competitive with respect to a state-of-the-art tool that extends the CVC4 solver with induction.",
isbn="978-3-030-51074-9"
}

@inproceedings{10.1145/2933575.2933578,
author = {D'Antoni, Loris and Veanes, Margus},
title = {Minimization of Symbolic Tree Automata},
year = {2016},
isbn = {9781450343916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2933575.2933578},
doi = {10.1145/2933575.2933578},
abstract = {Symbolic tree automata allow transitions to carry predicates over rich alphabet theories, such as linear arithmetic, and therefore extend finite tree automata to operate over infinite alphabets, such as the set of rational numbers. Existing tree automata algorithms rely on the alphabet being finite, and generalizing them to the symbolic setting is not a trivial task. In this paper we study the problem of minimizing symbolic tree automata. First, we formally define and prove the properties of minimality in the symbolic setting. Second, we lift existing minimization algorithms to symbolic tree automata. Third, we present a new algorithm based on the following idea: the problem of minimizing symbolic tree automata can be reduced to the problem of minimizing symbolic (string) automata by encoding the tree structure as part of the alphabet theory. We implement and evaluate all our algorithms against existing implementations and show that the symbolic algorithms scale to large alphabets and can minimize automata over complex alphabet theories.},
booktitle = {Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science},
pages = {873–882},
numpages = {10},
location = {New York, NY, USA},
series = {LICS '16}
}

@article{DBLP:journals/corr/abs-2304-12588,
  author       = {Shachar Itzhaky and
                  Sharon Shoham and
                  Yakir Vizel},
  title        = {Hyperproperty Verification as {CHC} Satisfiability},
  journal      = {CoRR},
  volume       = {abs/2304.12588},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2304.12588},
  doi          = {10.48550/arXiv.2304.12588},
  eprinttype    = {arXiv},
  eprint       = {2304.12588},
  timestamp    = {Wed, 03 May 2023 14:12:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2304-12588.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/978-3-031-13188-2_13,
author="Faella, Marco
and Parlato, Gennaro",
editor="Shoham, Sharon
and Vizel, Yakir",
title="Reasoning About Data Trees Using CHCs",
booktitle="Computer Aided Verification",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="249--271",
abstract="Reasoning about data structures requires powerful logics supporting the combination of structural and data properties. We define a new logic called Mso-D (Monadic Second-Order logic with Data) as an extension of standard Mso on trees with predicates of the desired data logic. We also define a new class of symbolic data tree automata (Sdtas) to deal with data trees using a simple machine. Mso-D and Sdtas are both Turing-powerful, and their high expressiveness is necessary to deal with interesting data structures. We cope with undecidability by encoding Sdta executions as a system of CHCs (Constrained Horn Clauses), and solving the resulting system using off-the-shelf solvers. We also identify a fragment of Mso-D whose satisfiability can be effectively reduced to the emptiness problem for Sdtas. This fragment is very expressive since it allows us to characterize a variety of data trees from the literature, solving certain infinite-state games, etc. We implement this reduction in a prototype tool that combines an Mso decision procedure over trees (Mona) with a CHC engine (Z3), and use this tool to conduct several experiments, demonstrating the effectiveness of our approach across different problem domains.",
isbn="978-3-031-13188-2"
}



@article{VEANES2015418,
title = {Symbolic tree automata},
journal = {Information Processing Letters},
volume = {115},
number = {3},
pages = {418-424},
year = {2015},
issn = {0020-0190},
doi = {https://doi.org/10.1016/j.ipl.2014.11.005},
author = {Margus Veanes and Nikolaj Bjørner},
keywords = {Tree automata, Algorithms, Logic, Satisfiability modulo theories, Formal methods},
abstract = {We introduce symbolic tree automata as a generalization of finite tree automata with a parametric alphabet over any given background theory. We show that symbolic tree automata are closed under Boolean operations, and that the operations are effectively uniform in the given alphabet theory. This generalizes the corresponding classical properties known for finite tree automata.}
}

@book{10.5555/267871,
editor = {Rozenberg, Grzegorz and Salomaa, Arto},
title = {Handbook of Formal Languages, Vol. 3: Beyond Words},
year = {1997},
isbn = {3540606491},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@InProceedings{losekoot_et_al:LIPIcs.FSCD.2023.7,
  author =	{Losekoot, Th\'{e}o and Genet, Thomas and Jensen, Thomas},
  title =	{{Automata-Based Verification of Relational Properties of Functions over Algebraic Data Structures}},
  booktitle =	{8th International Conference on Formal Structures for Computation and Deduction (FSCD 2023)},
  pages =	{7:1--7:22},
  series =	{Leibniz International Proceedings in Informatics (LIPIcs)},
  ISBN =	{978-3-95977-277-8},
  ISSN =	{1868-8969},
  year =	{2023},
  volume =	{260},
  editor =	{Gaboardi, Marco and van Raamsdonk, Femke},
  publisher =	{Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  address =	{Dagstuhl, Germany},
  URL =		{https://drops.dagstuhl.de/opus/volltexte/2023/17991},
  URN =		{urn:nbn:de:0030-drops-179915},
  doi =		{10.4230/LIPIcs.FSCD.2023.7},
  annote =	{Keywords: Formal verification, Tree automata, Constrained Horn Clauses, Model inference, Relational properties, Algebraic datatypes}
}

@article{10.1093/logcom/exab090,
    author = {De Angelis, Emanuele and Fioravanti, Fabio and Pettorossi, Alberto and Proietti, Maurizio},
    title = "{Satisfiability of constrained Horn clauses on algebraic data types: A transformation-based approach}",
    journal = {Journal of Logic and Computation},
    volume = {32},
    number = {2},
    pages = {402-442},
    year = {2022},
    month = {02},
    abstract = "{We address the problem of checking the satisfiability of constrained Horn clauses (CHCs) defined on algebraic data types (ADTs), such as lists and trees. We propose a new technique for transforming CHCs defined on ADTs into CHCs where the arguments of the predicates have only basic types, such as integers and booleans. Thus, our technique avoids, during satisfiability checking, the explicit use of proof rules based on induction over the ADTs. The main extension over previous techniques for ADT removal is a new transformation rule, called differential replacement, which allows us to introduce auxiliary predicates, whose definitions correspond to lemmas that are used when making inductive proofs. We present an algorithm that performs the automatic removal of ADTs by applying the new rule, together with the traditional folding/unfolding rules. We prove that, under suitable hypotheses, the set of the transformed clauses is satisfiable if and only if so is the set of the original clauses. By an experimental evaluation, we show that the use of the new rule significantly improves the effectiveness of ADT removal. We also show that our approach is competitive with respect to tools that extend CHC solvers with the use of inductive rules.}",
    issn = {0955-792X},
    doi = {10.1093/logcom/exab090},
    url = {https://doi.org/10.1093/logcom/exab090},
    eprint = {https://academic.oup.com/logcom/article-pdf/32/2/402/42618008/exab090.pdf},
}

@book{Kurshan1995,
title = {Computer-Aided Verification of Coordinating Processes},
title = {The Automata-Theoretic Approach},
author = {Robert P. Kurshan},
publisher = {Princeton University Press},
address = {Princeton},
doi = {doi:10.1515/9781400864041},
isbn = {9781400864041},
year = {1995},
lastchecked = {2022-12-03}
}

@InProceedings{10.1007/978-3-540-31980-1_1,
author="McMillan, K. L.",
editor="Halbwachs, Nicolas
and Zuck, Lenore D.",
title="Applications of Craig Interpolants in Model Checking",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--12",
abstract="A Craig interpolant for a mutually inconsistent pair of formulas (A,B) is a formula that is (1) implied by A, (2) inconsistent with B, and (3) expressed over the common variables of A and B. An interpolant can be efficiently derived from a refutation of A ∧ B, for certain theories and proof systems. We will discuss a number of applications of this concept in finite- and infinite-state model checking.",
isbn="978-3-540-31980-1"
}

@InProceedings{10.1007/978-3-540-45069-6_1,
author="McMillan, K. L.",
editor="Hunt, Warren A.
and Somenzi, Fabio",
title="Interpolation and SAT-Based Model Checking",
booktitle="Computer Aided Verification",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--13",
abstract="We consider a fully SAT-based method of unbounded symbolic model checking based on computing Craig interpolants. In benchmark studies using a set of large industrial circuit verification instances, this method is greatly more efficient than BDD-based symbolic model checking, and compares favorably to some recent SAT-based model checking methods on positive instances.",
isbn="978-3-540-45069-6"
}

@InProceedings{krishnan2020global,
author="Vediramana Krishnan, Hari Govind
and Chen, YuTing
and Shoham, Sharon
and Gurfinkel, Arie",
editor="Lahiri, Shuvendu K.
and Wang, Chao",
title="Global Guidance for Local Generalization in Model Checking",
booktitle="Computer Aided Verification",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="101--125",
abstract="SMT-based model checkers, especially IC3-style ones, are currently the most effective techniques for verification of infinite state systems. They infer global inductive invariants via local reasoning about a single step of the transition relation of a system, while employing SMT-based procedures, such as interpolation, to mitigate the limitations of local reasoning and allow for better generalization. Unfortunately, these mitigations intertwine model checking with heuristics of the underlying SMT-solver, negatively affecting stability of model checking.",
isbn="978-3-030-53291-8"
}

@article{loiseaux1995property,
  title={Property preserving abstractions for the verification of concurrent systems},
  author={Loiseaux, Claire and Graf, Susanne and Sifakis, Joseph and Bouajjani, Ahmed and Bensalem, Saddek and Probst, David},
  journal={Formal methods in system design},
  volume={6},
  pages={11--44},
  year={1995},
  publisher={Springer}
}

@InProceedings{cegar,
author="Clarke, Edmund
and Grumberg, Orna
and Jha, Somesh
and Lu, Yuan
and Veith, Helmut",
editor="Emerson, E. Allen
and Sistla, Aravinda Prasad",
title="Counterexample-Guided Abstraction Refinement",
booktitle="Computer Aided Verification",
year="2000",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="154--169",
abstract="We present an automatic iterative abstraction-refinement methodology in which the initial abstract model is generated by an automatic analysis of the control structures in the program to be verified. Abstract models may admit erroneous (or ``spurious'') counterexamples. We devise new symbolic techniques which analyze such counterexamples and refine the abstract model correspondingly. The refinement algorithm keeps the size of the abstract state space small due to the use of abstraction functions which distinguish many degrees of abstraction for each program variable. We describe an implementation of our methodology in NuSMV. Practical experiments including a large Fujitsu IP core design with about 500 latches and 10000 lines of SMV code confirm the effectiveness of our approach.",
isbn="978-3-540-45047-4"
}

@InProceedings{10.1007/3-540-49059-0_14,
author="Biere, Armin
and Cimatti, Alessandro
and Clarke, Edmund
and Zhu, Yunshan",
editor="Cleaveland, W. Rance",
title="Symbolic Model Checking without BDDs",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="1999",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="193--207",
abstract="Symbolic Model Checking [3], [14] has proven to be a powerful technique for the verification of reactive systems. BDDs [2] have traditionally been used as a symbolic representation of the system. In this paper we show how boolean decision procedures, like St{\aa}lmarck's Method [16] or the Davis {\&} Putnam Procedure [7], can replace BDDs. This new technique avoids the space blow up of BDDs, generates counterexamples much faster, and sometimes speeds up the verification. In addition, it produces counterexamples of minimal length. We introduce a bounded model checking procedure for LTL which reduces model checking to propositional satisfiability.We show that bounded LTL model checking can be done without a tableau construction. We have implemented a model checker BMC, based on bounded model checking, and preliminary results are presented.",
isbn="978-3-540-49059-3"
}

@InProceedings{10.1007/3-540-45657-0_40,
author="Stump, Aaron
and Barrett, Clark W.
and Dill, David L.",
editor="Brinksma, Ed
and Larsen, Kim Guldstrand",
title="CVC: A Cooperating Validity Checker",
booktitle="Computer Aided Verification",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="500--504",
abstract="Decision procedures for decidable logics and logical theories have proven to be useful tools in verification. This paper describes the CVC (``Cooperating Validity Checker'') decision procedure. CVC implements a framework for combining subsidiary decision procedures for certain logical theories into a decision procedure for the theories' union. Subsidiary decision procedures for theories of arrays, inductive datatypes, and linear real arithmetic are currently implemented. Other notable features of CVC are the incorporation of the high-performance Chaff solver for propositional reasoning, and the ability to produce independently checkable proofs for valid formulas.",
isbn="978-3-540-45657-5"
}

@InProceedings{10.1007/3-540-45757-7_26,
author="Tinelli, Cesare",
editor="Flesca, Sergio
and Greco, Sergio
and Ianni, Giovambattista
and Leone, Nicola",
title="A DPLL-Based Calculus for Ground Satisfiability Modulo Theories",
booktitle="Logics in Artificial Intelligence",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="308--319",
abstract="We describe and discuss DPLL(T ), a parametric calculus for proving the satisfiability of ground formulas in a logical theory T. The calculus tightly integrates a decision procedure for the satisfiability in T of sets of literals into a sequent calculus based on the well-known method by Davis, Putman, Logemann and Loveland for proving the satisfiability of propositional formulas. For being based on the DPLL method, DPLL(T ) can incorporate a number of very effective search heuristics developed by the SAT community for that method. Hence, it can be used as the formal basis for novel and efficient implementations of satisfiability checkers for theories with decidable ground consequences.",
isbn="978-3-540-45757-2"
}

@inproceedings{10.1145/378239.379017,
author = {Moskewicz, Matthew W. and Madigan, Conor F. and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
title = {Chaff: Engineering an Efficient SAT Solver},
year = {2001},
isbn = {1581132972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/378239.379017},
doi = {10.1145/378239.379017},
abstract = {Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search - especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.},
booktitle = {Proceedings of the 38th Annual Design Automation Conference},
pages = {530–535},
numpages = {6},
keywords = {design verification, boolean satisfiability},
location = {Las Vegas, Nevada, USA},
series = {DAC '01}
}

@inproceedings{silva1996grasp,
  title={GRASP-a new search algorithm for satisfiability.},
  author={Silva, Joao P Marques and Sakallah, Karem A},
  booktitle={ICCAD},
  volume={96},
  pages={220--227},
  year={1996},
  organization={Citeseer}
}

@inproceedings{10.5555/1864519.1864564,
author = {Kautz, Henry and Selman, Bart},
title = {Pushing the Envelope: Planning, Propositional Logic, and Stochastic Search},
year = {1996},
isbn = {026251091X},
publisher = {AAAI Press},
abstract = {Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very effective on a wide range of scheduling problems, this is the first demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning.},
booktitle = {Proceedings of the Thirteenth National Conference on Artificial Intelligence - Volume 2},
pages = {1194–1201},
numpages = {8},
location = {Portland, Oregon},
series = {AAAI'96}
}

@ARTICLE{1676819,
  author={Bryant},
  journal={IEEE Transactions on Computers}, 
  title={Graph-Based Algorithms for Boolean Function Manipulation}, 
  year={1986},
  volume={C-35},
  number={8},
  pages={677-691},
  doi={10.1109/TC.1986.1676819}}

@InProceedings{10.1007/3-540-61474-5_93,
author="Clarke, E.
and McMillan, K.
and Campos, S.
and Hartonas-Garmhausen, V.",
editor="Alur, Rajeev
and Henzinger, Thomas A.",
title="Symbolic model checking",
booktitle="Computer Aided Verification",
year="1996",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="419--422",
abstract="Symbolic model checking is a powerful formal specification and verification method that has been applied successfully in several industrial designs. Using symbolic model checking techniques it is possible to verify industrial-size finite state systems. State spaces with up to 1030 states can be exhaustively searched in minutes. Models with more than 10120 states have been verified using special techniques.",
isbn="978-3-540-68599-9"
}

@inproceedings{reger2017instantiation,
  title={Instantiation and Pretending to be an SMT Solver with Vampire.},
  author={Reger, Giles and Suda, Martin and Voronkov, Andrei},
  booktitle={SMT},
  pages={63--75},
  year={2017}
}

@article{10.2307/1990888,
 ISSN = {00029947},
 URL = {http://www.jstor.org/stable/1990888},
 author = {H. G. Rice},
 journal = {Transactions of the American Mathematical Society},
 number = {2},
 pages = {358--366},
 publisher = {American Mathematical Society},
 title = {Classes of Recursively Enumerable Sets and Their Decision Problems},
 urldate = {2022-12-03},
 volume = {74},
 year = {1953}
}

@InProceedings{10.1007/978-3-319-08867-9_5,
author="Garg, Pranav
and L{\"o}ding, Christof
and Madhusudan, P.
and Neider, Daniel",
editor="Biere, Armin
and Bloem, Roderick",
title="ICE: A Robust Framework for Learning Invariants",
booktitle="Computer Aided Verification",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="69--87",
abstract="We introduce ICE, a robust learning paradigm for synthesizing invariants, that learns using examples, counter-examples, and implications, and show that it admits honest teachers and strongly convergent mechanisms for invariant synthesis. We observe that existing algorithms for black-box abstract interpretation can be interpreted as ICE-learning algorithms. We develop new strongly convergent ICE-learning algorithms for two domains, one for learning Boolean combinations of numerical invariants for scalar variables and one for quantified invariants for arrays and dynamic lists. We implement these ICE-learning algorithms in a verification tool and show they are robust, practical, and efficient.",
isbn="978-3-319-08867-9"
}

@InProceedings{10.1007/978-3-642-31612-8_13,
author="Hoder, Kry{\v{s}}tof
and Bj{\o}rner, Nikolaj",
editor="Cimatti, Alessandro
and Sebastiani, Roberto",
title="Generalized Property Directed Reachability",
booktitle="Theory and Applications of Satisfiability Testing -- SAT 2012",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="157--171",
abstract="The IC3 algorithm was recently introduced for proving properties of finite state reactive systems. It has been applied very successfully to hardware model checking. We provide a specification of the algorithm using an abstract transition system and highlight its dual operation: model search and conflict resolution. We then generalize it along two dimensions. Along one dimension we address nonlinear fixed-point operators (push-down systems) and evaluate the algorithm on Boolean programs. In the second dimension we leverage proofs and models and generalize the method to Boolean constraints involving theories.",
isbn="978-3-642-31612-8"
}

@InProceedings{10.1007/978-3-642-54862-8_4,
author="Cimatti, Alessandro
and Griggio, Alberto
and Mover, Sergio
and Tonetta, Stefano",
editor="{\'A}brah{\'a}m, Erika
and Havelund, Klaus",
title="IC3 Modulo Theories via Implicit Predicate Abstraction",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="46--61",
abstract="We present a novel approach for generalizing the IC3 algorithm for invariant checking from finite-state to infinite-state transition systems, expressed over some background theories. The procedure is based on a tight integration of IC3 with Implicit (predicate) Abstraction, a technique that expresses abstract transitions without computing explicitly the abstract system and is incremental with respect to the addition of predicates. In this scenario, IC3 operates only at the Boolean level of the abstract state space, discovering inductive clauses over the abstraction predicates. Theory reasoning is confined within the underlying SMT solver, and applied transparently when performing satisfiability checks. When the current abstraction allows for a spurious counterexample, it is refined by discovering and adding a sufficient set of new predicates. Importantly, this can be done in a completely incremental manner, without discarding the clauses found in the previous search.",
isbn="978-3-642-54862-8"
}

@InProceedings{10.1007/978-3-642-18275-4_7,
author="Bradley, Aaron R.",
editor="Jhala, Ranjit
and Schmidt, David",
title="SAT-Based Model Checking without Unrolling",
booktitle="Verification, Model Checking, and Abstract Interpretation",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="70--87",
abstract="A new form of SAT-based symbolic model checking is described. Instead of unrolling the transition relation, it incrementally generates clauses that are inductive relative to (and augment) stepwise approximate reachability information. In this way, the algorithm gradually refines the property, eventually producing either an inductive strengthening of the property or a counterexample trace. Our experimental studies show that induction is a powerful tool for generalizing the unreachability of given error states: it can refine away many states at once, and it is effective at focusing the proof search on aspects of the transition system relevant to the property. Furthermore, the incremental structure of the algorithm lends itself to a parallel implementation.",
isbn="978-3-642-18275-4"
}

@inbook{10.1007/978-3-540-69850-0_1,
author = {Clarke, Edmund M.},
title = {The Birth of Model Checking},
year = {2008},
isbn = {9783540698494},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-540-69850-0_1},
abstract = {"When the time is ripe for certain things, these things appear in different places in the manner of violets coming to light in early spring." (Wolfgang Bolyai to his son Johann in urging him to claim the invention of non- Euclidean geometry without delay [Vit88]).},
booktitle = {25 Years of Model Checking: History, Achievements, Perspectives},
pages = {1–26},
numpages = {26}
}

@InProceedings{10.1007/BFb0025774,
author="Clarke, Edmund M.
and Emerson, E. Allen",
editor="Kozen, Dexter",
title="Design and synthesis of synchronization skeletons using branching time temporal logic",
booktitle="Logics of Programs",
year="1982",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="52--71",
abstract="We have shown that it is possible to automatically synthesize the synchronization skeleton of a concurrent program from a Temporal Logic specification. We believe that this approach may in the long run turn out to be quite practical. Since synchronization skeletons are, in general, quite small, the potentially exponential behavior of our algorithm need not be an insurmountable obstacle. Much additional research will be needed, however, to make the approach feasible in practice.",
isbn="978-3-540-39047-3"
}

@article{turing1936computable,
  title={On computable numbers, with an application to the Entscheidungsproblem},
  author={Turing, Alan Mathison and others},
  journal={J. of Math},
  volume={58},
  number={345-363},
  pages={5},
  year={1936}
}

@Inbook{Clarke2018,
author="Clarke, Edmund M.
and Henzinger, Thomas A.
and Veith, Helmut",
editor="Clarke, Edmund M.
and Henzinger, Thomas A.
and Veith, Helmut
and Bloem, Roderick",
title="Introduction to Model Checking",
bookTitle="Handbook of Model Checking",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="1--26",
abstract="Model checking is a computer-assisted method for the analysis of dynamical systems that can be modeled by state-transition systems. Drawing from research traditions in mathematical logic, programming languages, hardware design, and theoretical computer science, model checking is now widely used for the verification of hardware and software in industry. This chapter is an introduction and short survey of model checking. The chapter aims to motivate and link the individual chapters of the handbook, and to provide context for readers who are not familiar with model checking.",
isbn="978-3-319-10575-8",
doi="10.1007/978-3-319-10575-8_1",
url="https://doi.org/10.1007/978-3-319-10575-8_1"
}

@article{bjorner2015playing,
  title={Playing with Quantified Satisfaction.},
  author={Bj{\o}rner, Nikolaj S and Janota, Mikol{\'a}s},
  journal={LPAR (short papers)},
  volume={35},
  pages={15--27},
  year={2015}
}

@article{10.1145/363235.363259,
author = {Hoare, C. A. R.},
title = {An Axiomatic Basis for Computer Programming},
year = {1969},
issue_date = {Oct. 1969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/363235.363259},
doi = {10.1145/363235.363259},
abstract = {In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics.},
journal = {Commun. ACM},
month = {oct},
pages = {576–580},
numpages = {5},
keywords = {formal language definition, program documentation, machine-independent programming, programming language design, theory of programming' proofs of programs, axiomatic method}
}

@article{brady_2013, title={Idris, a general-purpose dependently typed programming language: Design and implementation}, volume={23}, DOI={10.1017/S095679681300018X}, number={5}, journal={Journal of Functional Programming}, publisher={Cambridge University Press}, author={Brady, Edwin}, year={2013}, pages={552–593}}

@InProceedings{10.1007/978-3-030-02768-1_8,
author="Champion, Adrien
and Kobayashi, Naoki
and Sato, Ryosuke",
editor="Ryu, Sukyoung",
title="HoIce: An ICE-Based Non-linear Horn Clause Solver",
booktitle="Programming Languages and Systems",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="146--156",
abstract="The ICE framework is a machine-learning-based technique originally introduced for inductive invariant inference over transition systems, and building on the supervised learning paradigm. Recently, we adapted the approach to non-linear Horn clause solving in the context of higher-order program verification. We showed that we could solve more of our benchmarks (extracted from higher-order program verification problems) than other state-of-the-art Horn clause solvers. This paper discusses some of the many improvements we recently implemented in HoIce, our implementation of this generalized ICE framework.",
isbn="978-3-030-02768-1"
}

@article{BARHILLELPERLESSHAMIR,
url = {https://doi.org/10.1524/stuf.1961.14.14.143},
title = {On formal properties of simple phrase structure grammars},
author={Bar-Hillel, Yehoshua and Perles, Micha and Shamir, Eli},
pages = {143--172},
volume = {14},
number = {1-4},
journal = {STUF - Language Typology and Universals},
doi = {doi:10.1524/stuf.1961.14.14.143},
year = {1961},
lastchecked = {2023-08-12}
}


@article{komuravelli2016smt,
  title={SMT-based model checking for recursive programs},
  author={Komuravelli, Anvesh and Gurfinkel, Arie and Chaki, Sagar},
  journal={Formal Methods in System Design},
  volume={48},
  number={3},
  pages={175--205},
  year={2016},
  publisher={Springer}
}

@article{De_Angelis_2022,
	doi = {10.4204/eptcs.373.5},
  
	url = {https://doi.org/10.4204%2Feptcs.373.5},
  
	year = 2022,
	month = {nov},
  
	publisher = {Open Publishing Association},
  
	volume = {373},
  
	pages = {44--62},
  
	author = {Emanuele De Angelis and Hari Govind V K},
  
	title = {{CHC}-{COMP} 2022: Competition Report},
  
	journal = {Electronic Proceedings in Theoretical Computer Science}
}

@InProceedings{10.1007/978-3-030-79876-5_37,
author="Moura, Leonardo de
and Ullrich, Sebastian",
editor="Platzer, Andr{\'e}
and Sutcliffe, Geoff",
title="The Lean 4 Theorem Prover and Programming Language",
booktitle="Automated Deduction -- CADE 28",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="625--635",
abstract="Lean 4 is a reimplementation of the Lean interactive theorem prover (ITP) in Lean itself. It addresses many shortcomings of the previous versions and contains many new features. Lean 4 is fully extensible: users can modify and extend the parser, elaborator, tactics, decision procedures, pretty printer, and code generator. The new system has a hygienic macro system custom-built for ITPs. It contains a new typeclass resolution procedure based on tabled resolution, addressing significant performance problems reported by the growing user base. Lean 4 is also an efficient functional programming language based on a novel programming paradigm called functional but in-place. Efficient code generation is crucial for Lean users because many write custom proof automation procedures in Lean itself.",
isbn="978-3-030-79876-5"
}

@article{10.1145/3341691,
author = {Vezzosi, Andrea and M\"{o}rtberg, Anders and Abel, Andreas},
title = {Cubical Agda: A Dependently Typed Programming Language with Univalence and Higher Inductive Types},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {ICFP},
url = {https://doi.org/10.1145/3341691},
doi = {10.1145/3341691},
abstract = {Proof assistants based on dependent type theory provide expressive languages for both programming and proving within the same system. However, all of the major implementations lack powerful extensionality principles for reasoning about equality, such as function and propositional extensionality. These principles are typically added axiomatically which disrupts the constructive properties of these systems. Cubical type theory provides a solution by giving computational meaning to Homotopy Type Theory and Univalent Foundations, in particular to the univalence axiom and higher inductive types. This paper describes an extension of the dependently typed functional programming language Agda with cubical primitives, making it into a full-blown proof assistant with native support for univalence and a general schema of higher inductive types. These new primitives make function and propositional extensionality as well as quotient types directly definable with computational content. Additionally, thanks also to copatterns, bisimilarity is equivalent to equality for coinductive types. This extends Agda with support for a wide range of extensionality principles, without sacrificing type checking and constructivity.},
journal = {Proc. ACM Program. Lang.},
month = {jul},
articleno = {87},
numpages = {29},
keywords = {Dependent Pattern Matching, Higher Inductive Types, Univalence, Cubical Type Theory}
}

@techreport{barras1999coq,
  TITLE = {{The Coq Proof Assistant: Reference Manual : Version 7.2}},
  author={Barras, Bruno and Boutin, Samuel and Cornes, Cristina and Courant, Judica{\"e}l and Coscoy, Yann and Delahaye, David and de Rauglaudre, Daniel and Filli{\^a}tre, Jean-Christophe and Gim{\'e}nez, Eduardo and Herbelin, Hugo and others},
  URL = {https://inria.hal.science/inria-00069919},
  NUMBER = {RT-0255},
  PAGES = {290},
  INSTITUTION = {{INRIA}},
  YEAR = {2002},
  MONTH = Feb,
  KEYWORDS = {COQ ; PROOF ASSISTANT ; FORMAL PROOFS ; CALCULUS OF INDUCTIVE CONSTRUCTIONS},
  PDF = {https://inria.hal.science/inria-00069919/file/RT-0255.pdf},
  HAL_ID = {inria-00069919},
  HAL_VERSION = {v1},
}

@article{10.1145/2090147.2094081,
author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
title = {SAGE: Whitebox Fuzzing for Security Testing: SAGE Has Had a Remarkable Impact at Microsoft.},
year = {2012},
issue_date = {January 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {1},
issn = {1542-7730},
url = {https://doi.org/10.1145/2090147.2094081},
doi = {10.1145/2090147.2094081},
abstract = {Most ACM Queue readers might think of "program verification research" as mostly theoretical with little impact on the world at large. Think again. If you are reading these lines on a PC running some form of Windows (like 93-plus percent of PC users--that is, more than a billion people), then you have been affected by this line of work--without knowing it, which is precisely the way we want it to be.},
journal = {Queue},
month = {jan},
pages = {20–27},
numpages = {8}
}

@inproceedings{reger2014challenges,
  title={The Challenges of Evaluating a New Feature in Vampire.},
  author={Reger, Giles and Suda, Martin and Voronkov, Andrei},
  booktitle={Vampire Workshop},
  pages={70--74},
  year={2014}
}

@article{giacobazzi2015analyzing,
  title={Analyzing program analyses},
  author={Giacobazzi, Roberto and Logozzo, Francesco and Ranzato, Francesco},
  journal={ACM SIGPLAN Notices},
  volume={50},
  number={1},
  pages={261--273},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@phdthesis{mordvinov2020,
  title={Автоматический вывод реляционных инвариантов для нелинейных систем дизъюнктов Хорна с ограничениями},
  author={Мордвинов, Дмитрий Александрович},
  year={2020},
  school={Санкт-Петербургский государственный университет},
  language = {russian}
}

@InProceedings{kovacs2013first,
author="Kov{\'a}cs, Laura
and Voronkov, Andrei",
editor="Sharygina, Natasha
and Veith, Helmut",
title="First-Order Theorem Proving and Vampire",
booktitle="Computer Aided Verification",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--35",
abstract="In this paper we give a short introduction in first-order theorem proving and the use of the theorem prover Vampire. We discuss the superposition calculus and explain the key concepts of saturation and redundancy elimination, present saturation algorithms and preprocessing, and demonstrate how these concepts are implemented in Vampire. Further, we also cover more recent topics and features of Vampire designed for advanced applications, including satisfiability checking, theory reasoning, interpolation, consequence elimination, and program analysis.",
isbn="978-3-642-39799-8"
}

@InProceedings{10.1007/978-3-319-66167-4_10,
author="Cruanes, Simon",
editor="Dixon, Clare
and Finger, Marcelo",
title="Superposition with Structural Induction",
booktitle="Frontiers of Combining Systems",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="172--188",
abstract="Superposition-based provers have been successfully used to discharge proof obligations stemming from proof assistants. However, many such obligations require induction to be proved. We present a new extension of typed superposition that can perform structural induction. Several inductive goals can be attempted within a single saturation loop, by leveraging {\$}{\$}{\backslash}text {\{}AVATAR{\}}{\$}{\$} [1]. Lemmas obtained by generalization or theory exploration can be introduced during search, used, and proved, all in the same search space. We describe an implementation and present some promising results.",
isbn="978-3-319-66167-4"
}

@INPROCEEDINGS{4556689,
  author={Goubault-Larrecq, Jean},
  booktitle={2008 21st IEEE Computer Security Foundations Symposium}, 
  title={Towards Producing Formally Checkable Security Proofs, Automatically}, 
  year={2008},
  volume={},
  number={},
  pages={224-238},
  doi={10.1109/CSF.2008.21}}

@article{peltier2009constructing,
  title={Constructing infinite models represented by tree automata},
  author={Peltier, Nicolas},
  journal={Annals of Mathematics and Artificial Intelligence},
  volume={56},
  number={1},
  pages={65--85},
  year={2009},
  publisher={Springer}
}

@book{kozen2012automata,
  title={Automata and Computability},
  author={Kozen, D.C.},
  isbn={9781461218449},
  lccn={96037409},
  series={Undergraduate Texts in Computer Science},
  url={https://books.google.ru/books?id=Vo3fBwAAQBAJ},
  year={2012},
  publisher={Springer New York}
}

@InProceedings{lisitsa2012finite,
  author =	{Alexei Lisitsa},
  title =	{{Finite Models vs Tree Automata in Safety Verification}},
  booktitle =	{23rd International Conference on Rewriting Techniques and Applications (RTA'12) },
  pages =	{225--239},
  series =	{Leibniz International Proceedings in Informatics (LIPIcs)},
  ISBN =	{978-3-939897-38-5},
  ISSN =	{1868-8969},
  year =	{2012},
  volume =	{15},
  editor =	{Ashish Tiwari},
  publisher =	{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  address =	{Dagstuhl, Germany},
  URL =		{http://drops.dagstuhl.de/opus/volltexte/2012/3495},
  URN =		{urn:nbn:de:0030-drops-34959},
  doi =		{10.4230/LIPIcs.RTA.2012.225},
  annote =	{Keywords: term-rewriting systems, safety verification, first-order logic, finite model finding}
}

@InProceedings{princess,
author="R{\"u}mmer, Philipp",
editor="Cervesato, Iliano
and Veith, Helmut
and Voronkov, Andrei",
title="A Constraint Sequent Calculus for First-Order Logic with Linear Integer Arithmetic",
booktitle="Logic for Programming, Artificial Intelligence, and Reasoning",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="274--289",
abstract="First-order logic modulo the theory of integer arithmetic is the basis for reasoning in many areas, including deductive software verification and software model checking. While satisfiability checking for ground formulae in this logic is well understood, it is still an open question how the general case of quantified formulae can be handled in an efficient and systematic way. As a possible answer, we introduce a sequent calculus that combines ideas from free-variable constraint tableaux with the Omega quantifier elimination procedure. The calculus is complete for theorems of first-order logic (without functions, but with arbitrary uninterpreted predicates), can decide Presburger arithmetic, and is complete for a substantial fragment of the combination of both.",
isbn="978-3-540-89439-1"
}

@TECHREPORT{BarFT-RR-17,
  author =	 {Clark Barrett and Pascal Fontaine and Cesare Tinelli},
  title =	 {{The SMT-LIB Standard: Version 2.6}},
  institution =	 {Department of Computer Science, The University of Iowa},
  year =	 2017,
  note =	 {Available at http://smtlib.cs.uiowa.edu/}
}

@InProceedings{cvc5,
author="Barbosa, Haniel
and Barrett, Clark
and Brain, Martin
and Kremer, Gereon
and Lachnitt, Hanna
and Mann, Makai
and Mohamed, Abdalrhman
and Mohamed, Mudathir
and Niemetz, Aina
and N{\"o}tzli, Andres
and Ozdemir, Alex
and Preiner, Mathias
and Reynolds, Andrew
and Sheng, Ying
and Tinelli, Cesare
and Zohar, Yoni",
editor="Fisman, Dana
and Rosu, Grigore",
title="cvc5: A Versatile and Industrial-Strength SMT Solver",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="415--442",
abstract="cvc5 is the latest SMT solver in the cooperating validity checker series and builds on the successful code base of CVC4. This paper serves as a comprehensive system description of cvc5 's architectural design and highlights the major features and components introduced since CVC4 1.8. We evaluate cvc5's performance on all benchmarks in SMT-LIB and provide a comparison against CVC4 and Z3.",
isbn="978-3-030-99524-9"
}

@InProceedings{de2008z3,
author="de Moura, Leonardo
and Bj{\o}rner, Nikolaj",
editor="Ramakrishnan, C. R.
and Rehof, Jakob",
title="Z3: An Efficient SMT Solver",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="337--340",
abstract="Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.",
isbn="978-3-540-78800-3"
}

@article{10.5555/1218615.1218621,
author = {Schulz, Stephan},
title = {E - a Brainiac Theorem Prover},
year = {2002},
issue_date = {August 2002},
publisher = {IOS Press},
address = {NLD},
volume = {15},
number = {2,3},
issn = {0921-7126},
abstract = {We describe the superposition-based theorem prover E. E is a sound and complete prover for clausal first order logic with equality. Important properties of the prover include strong redundancy elimination criteria, the DISCOUNT loop proof procedure, a very flexible interface for specifying search control heuristics, and an efficient inference engine. We also discuss strength and weaknesses of the system.},
journal = {AI Commun.},
month = {aug},
pages = {111–126},
numpages = {16},
keywords = {Theorem proving, superposition, rewriting, search control, E}
}

@InProceedings{10.1007/978-3-319-40970-2_20,
author="Reger, Giles
and Suda, Martin
and Voronkov, Andrei",
editor="Creignou, Nadia
and Le Berre, Daniel",
title="Finding Finite Models in Multi-sorted First-Order Logic",
booktitle="Theory and Applications of Satisfiability Testing -- SAT 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="323--341",
abstract="This work extends the existing MACE-style finite model finding approach to multi-sorted first-order logic. This existing approach iteratively assumes increasing domain sizes and encodes the related ground problem as a SAT problem. When moving to the multi-sorted setting each sort may have a different domain size, leading to an explosion in the search space. This paper focusses on methods to tame that search space. The key approach adds additional information to the SAT encoding to suggest which domains should be grown. Evaluation of an implementation of techniques in the Vampire theorem prover shows that they dramatically reduce the search space and that this is an effective approach to find finite models in multi-sorted first-order logic.",
isbn="978-3-319-40970-2"
}

@InProceedings{claessen2003new,
  title={New techniques that improve MACE-style finite model finding},
  author={Claessen, Koen and S{\"o}rensson, Niklas},
  booktitle={Proceedings of the CADE-19 Workshop: Model Computation-Principles, Algorithms, Applications},
  pages={11--27},
  year={2003},
  organization={Citeseer}
}

@InProceedings{reynolds2013finite,
author="Reynolds, Andrew
and Tinelli, Cesare
and Goel, Amit
and Krsti{\'{c}}, Sava",
editor="Sharygina, Natasha
and Veith, Helmut",
title="Finite Model Finding in SMT",
booktitle="Computer Aided Verification",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="640--655",
abstract="SMT solvers have been used successfully as reasoning engines for automated verification. Current techniques for dealing with quantified formulas in SMT are generally incomplete, forcing SMT solvers to report ``unknown'' when they fail to prove the unsatisfiability of a formula with quantifiers. This inability to return counter-models limits their usefulness in applications that produce quantified verification conditions. We present a novel finite model finding method that reduces these limitations in the case of quantifiers ranging over free sorts. Our method contrasts with previous approaches for finite model finding in first-order logic by not relying on the introduction of domain constants for the free sorts and by being fully integrated into the general architecture used by most SMT solvers. This integration is achieved through the addition of a novel solver for sort cardinality constraints and a module for quantifier instantiation over finite domains. Initial experiments with verification conditions generated from a deductive verification tool developed at Intel Corp. show that our approach compares quite favorably with the state of the art in SMT.",
isbn="978-3-642-39799-8"
}

@InProceedings{10.1007/978-3-540-71209-1_49,
author="Torlak, Emina
and Jackson, Daniel",
editor="Grumberg, Orna
and Huth, Michael",
title="Kodkod: A Relational Model Finder",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="632--647",
abstract="The key design challenges in the construction of a SAT-based relational model finder are described, and novel techniques are proposed to address them. An efficient model finder must have a mechanism for specifying partial solutions, an effective symmetry detection and breaking scheme, and an economical translation from relational to boolean logic. These desiderata are addressed with three new techniques: a symmetry detection algorithm that works in the presence of partial solutions, a sparse-matrix representation of relations, and a compact representation of boolean formulas inspired by boolean expression diagrams and reduced boolean circuits. The presented techniques have been implemented and evaluated, with promising results.",
isbn="978-3-540-71209-1"
}

@InProceedings{10.1007/3-540-58156-1_63,
author="Slaney, John",
editor="Bundy, Alan",
title="FINDER: Finite domain enumerator system description",
booktitle="Automated Deduction --- CADE-12",
year="1994",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="798--801",
isbn="978-3-540-48467-7"
}

@misc{https://doi.org/10.48550/arxiv.cs/0310055,
  doi = {10.48550/ARXIV.CS/0310055},
  
  url = {https://arxiv.org/abs/cs/0310055},
  
  author = {McCune, William},
  
  keywords = {Symbolic Computation (cs.SC), Mathematical Software (cs.MS), FOS: Computer and information sciences, FOS: Computer and information sciences, F.4.1},
  
  title = {Mace4 Reference Manual and Guide},
  
  publisher = {arXiv},
  
  year = {2003},
  
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004}
}

@article{angelis_fioravanti_pettorossi_proietti_2015, title={Proving correctness of imperative programs by linearizing constrained Horn clauses}, volume={15}, DOI={10.1017/S1471068415000289}, number={4-5}, journal={Theory and Practice of Logic Programming}, publisher={Cambridge University Press}, author={De Angelis, Emanuele and Fioravanti, Fabio and Pettorossi, Alberto and Proietti, Maurizio}, year={2015}, pages={635–650}}

@InProceedings{10.1007/978-3-662-53413-7_8,
author="De Angelis, Emanuele
and Fioravanti, Fabio
and Pettorossi, Alberto
and Proietti, Maurizio",
editor="Rival, Xavier",
title="Relational Verification Through Horn Clause Transformation",
booktitle="Static Analysis",
year="2016",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="147--169",
abstract="We present a method for verifying relational program properties, that is, properties that relate the input and the output of two programs. Our verification method is parametric with respect to the definition of the operational semantics of the programming language in which the two programs are written. That definition of the semantics consists of a set Int of constrained Horn clauses (CHCs) that encode the interpreter of the programming language. Then, given the programs and the relational property we want to verify, we generate, by using Int, a set of constrained Horn clauses whose satisfiability is equivalent to the validity of the property. Unfortunately, state-of-the-art solvers for CHCs have severe limitations in proving the satisfiability, or the unsatisfiability, of such sets of clauses. We propose some transformation techniques that increase the power of CHC solvers when verifying relational properties. We show that these transformations, based on unfold/fold rules, preserve satisfiability. Through an experimental evaluation, we show that in many cases CHC solvers are able to prove the satisfiability (or the unsatisfiability) of sets of clauses obtained by applying the transformations we propose, whereas the same solvers are unable to perform those proofs when given as input the original, untransformed sets of CHCs.",
isbn="978-3-662-53413-7"
}

@inproceedings{LPAR-21:Synchronizing_Constrained_Horn_Clauses,
  author    = {Dmitry Mordvinov and Grigory Fedyukovich},
  title     = {Synchronizing Constrained Horn Clauses},
  booktitle = {LPAR-21. 21st International Conference on Logic for Programming, Artificial Intelligence and Reasoning},
  editor    = {Thomas Eiter and David Sands},
  series    = {EPiC Series in Computing},
  volume    = {46},
  pages     = {338--355},
  year      = {2017},
  publisher = {EasyChair},
  bibsource = {EasyChair, https://easychair.org},
  issn      = {2398-7340},
  url       = {https://easychair.org/publications/paper/LlxW},
  doi       = {10.29007/gr5c}}

@InProceedings{fermuller2005model,
author="Ferm{\"u}ller, Christian G.
and Pichler, Reinhard",
editor="Nieuwenhuis, Robert",
title="Model Representation via Contexts and Implicit Generalizations",
booktitle="Automated Deduction -- CADE-20",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="409--423",
abstract="Some results on expressibility and complexity issues in model representation are presented. In particular, the relation between so-called `contexts' as recently introduced in [4] for the model evolution calculus and the more traditional `disjunctions of implicit generalizations' (DIGs) [21] is clarified: contexts are as expressible as DIGs, but DIGs may represent the same model exponentially more succinctly. The clause evaluation problem and the equivalence problem for DIGs and contexts, respectively, are all shown to be coNP-complete.",
isbn="978-3-540-31864-4"
}

@book{caferra2013automated,
  title={Automated model building},
  author={Caferra, Ricardo and Leitsch, Alexander and Peltier, Nicolas},
  volume={31},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@InProceedings{teucke2019expressivity,
author="Teucke, Andreas
and Voigt, Marco
and Weidenbach, Christoph",
editor="Herzig, Andreas
and Popescu, Andrei",
title="On the Expressivity and Applicability of Model Representation Formalisms",
booktitle="Frontiers of Combining Systems",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="22--39",
abstract="A number of first-order calculi employ an explicit model representation formalism in support of non-redundant inferences and for detecting satisfiability. Many of these formalisms can represent infinite Herbrand models. The first-order fragment of monadic, shallow, linear, Horn (MSLH) clauses, is such a formalism used in the approximation refinement calculus (AR). Our first result is a finite model property for MSLH clause sets. Therefore, MSLH clause sets cannot represent models of clause sets with inherently infinite models. Through a translation to tree automata, we further show that this limitation also applies to the linear fragments of implicit generalizations, which is the formalism used in the model-evolution calculus (ME), to atoms with disequality constraints, the formalisms used in the non-redundant clause learning calculus (NRCL), and to atoms with membership constraints, a formalism used for example in decision procedures for algebraic data types. Although these formalisms cannot represent models of clause sets with inherently infinite models, through an additional approximation step they can. This is our second main result. For clause sets including the definition of an equivalence relation with the help of an additional, novel approximation, called reflexive relation splitting, the approximation refinement calculus can automatically show satisfiability through the MSLH clause set formalism.",
isbn="978-3-030-29007-8"
}

@InProceedings{gramlich2002algorithmic,
author="Gramlich, Bernhard
and Pichler, Reinhard",
editor="Voronkov, Andrei",
title="Algorithmic Aspects of Herbrand Models Represented by Ground Atoms with Ground Equations",
booktitle="Automated Deduction---CADE-18",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="241--259",
abstract="Automated model building has evolved as an important sub-discipline of automated deduction over the past decade. One crucial issue in automated model building is the selection of an appropriate (finite) representation of (in general infinite) models. Quite a few such formalisms have been proposed in the literature. In this paper, we concentrate on the representation of Herbrand models by ground atoms with ground equations (GAE-models), introduced in [9]. For the actual work with any model representation, efficient algorithms for two decision problems are required, namely: The clause evaluation problem (i.e.: Given a clause C and a representation {\$}{\$}{\backslash}mathcal{\{}M{\}}{\$}{\$}of a model, does C evaluate to ``true'' in this model?) and the model equivalence problem (i.e.: Given two representations {\$}{\$}{\backslash}mathcal{\{}M{\}}{\_}1 {\$}{\$}and {\$}{\$}{\backslash}mathcal{\{}M{\}}{\_}2 {\$}{\$}, do they represent the same model?). Previously published algorithms for these two problems in case of GAE-models require exponential time. We prove that the clause evaluation problem is indeed intractable (that is, coNP-complete), whereas the model equivalence problem can be solved in polynomial time. Moreover, we show how our new algorithm for the model equivalence problem can be used to transform an arbitrary GAE-model into an equivalent one with better computational properties.",
isbn="978-3-540-45620-9"
}

@ARTICLE{fermuller2007model,
  author={Fermüller, Christian G. and Pichler, Reinhard},
  journal={Journal of Logic and Computation}, 
  title={Model Representation over Finite and Infinite Signatures}, 
  year={2007},
  volume={17},
  number={3},
  pages={453-477},
  doi={10.1093/logcom/exm008}
}

@inproceedings{10.1145/2837614.2837640,
author = {Padon, Oded and Immerman, Neil and Shoham, Sharon and Karbyshev, Aleksandr and Sagiv, Mooly},
title = {Decidability of Inferring Inductive Invariants},
year = {2016},
isbn = {9781450335492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837614.2837640},
doi = {10.1145/2837614.2837640},
abstract = {Induction is a successful approach for verification of hardware and software systems. A common practice is to model a system using logical formulas, and then use a decision procedure to verify that some logical formula is an inductive safety invariant for the system. A key ingredient in this approach is coming up with the inductive invariant, which is known as invariant inference. This is a major difficulty, and it is often left for humans or addressed by sound but incomplete abstract interpretation. This paper is motivated by the problem of inductive invariants in shape analysis and in distributed protocols. This paper approaches the general problem of inferring first-order inductive invariants by restricting the language L of candidate invariants. Notice that the problem of invariant inference in a restricted language L differs from the safety problem, since a system may be safe and still not have any inductive invariant in L that proves safety. Clearly, if L is finite (and if testing an inductive invariant is decidable), then inferring invariants in L is decidable. This paper presents some interesting cases when inferring inductive invariants in L is decidable even when L is an infinite language of universal formulas. Decidability is obtained by restricting L and defining a suitable well-quasi-order on the state space. We also present some undecidability results that show that our restrictions are necessary. We further present a framework for systematically constructing infinite languages while keeping the invariant inference problem decidable. We illustrate our approach by showing the decidability of inferring invariants for programs manipulating linked-lists, and for distributed protocols.},
booktitle = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {217–231},
numpages = {15},
keywords = {invariant inference, well-quasi-order, effectively propositional logic, verification},
location = {St. Petersburg, FL, USA},
series = {POPL '16}
}

@INPROCEEDINGS{9470608,
  author={Bruni, Roberto and Giacobazzi, Roberto and Gori, Roberta and Ranzato, Francesco},
  booktitle={2021 36th Annual ACM/IEEE Symposium on Logic in Computer Science (LICS)}, 
  title={A Logic for Locally Complete Abstract Interpretations}, 
  year={2021},
  volume={},
  number={},
  pages={1-13},
  doi={10.1109/LICS52264.2021.9470608}}

@article{10.1093/logcom/2.4.511,
    author={Cousot, Patrick and Cousot, Radhia},
    title = "{Abstract Interpretation Frameworks}",
    journal = {Journal of Logic and Computation},
    volume = {2},
    number = {4},
    pages = {511-547},
    year = {1992},
    month = {08},
    abstract = "{We introduce abstract interpretation frameworks which are variations on the archetypal framework using Galois connections between concrete and abstract semantics, widenings and narrowings and are obtained by relaxation of the original hypotheses. We consider various ways of establishing the correctness of an abstract interpretation depending on how the relation between the concrete and abstract semantics is denned. We insist upon those correspondences allowing for the inducing of the approximate abstract semantics from the concrete one. Furthermore we study various notions of widening and narrowing as a means of obtaining convergence in the iterations used in abstract interpretation.}",
    issn = {0955-792X},
    doi = {10.1093/logcom/2.4.511},
    url = {https://doi.org/10.1093/logcom/2.4.511},
    eprint = {https://academic.oup.com/logcom/article-pdf/2/4/511/2740133/2-4-511.pdf},
}

@article{10.1145/333979.333989,
author = {Giacobazzi, Roberto and Ranzato, Francesco and Scozzari, Francesca},
title = {Making Abstract Interpretations Complete},
year = {2000},
issue_date = {March 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/333979.333989},
doi = {10.1145/333979.333989},
abstract = {Completeness is an ideal, although uncommon, feature of abstract interpretations, formalizing the intuition that, relatively to the properties encoded by the underlying abstract domains, there is no loss of information accumulated in abstract computations. Thus, complete abstract interpretations can be rightly understood as optimal. We deal with both pointwise completeness, involving generic semantic operations, and (least) fixpoint completeness. Completeness and fixpoint completeness are shown to be properties that depend on the underlying abstract domains only. Our primary goal is then to solve the problem of making abstract interpretations complete by minimally extending or restricting the underlying abstract domains. Under the weak and reasonable hypothesis of dealing with continuous semantic operations, we provide constructive characterizations for the least complete extensions and the greatest complete restrictions of abstract domains. As far as fixpoint completeness is concerned, for merely monotone semantic operators, the greatest restrictions of abstract domains are constructively characterized, while it is shown that the existence of least extensions of abstract domains cannot be, in general, guaranteed, even under strong hypotheses. These methodologies, which in finite settings give rise to effective algorithms, provide advanced formal tools for manipulating and comparing abstract interpretations, useful both in static program analysis and in semantics design. A number of examples illustrating these techniques are given.},
journal = {J. ACM},
month = {mar},
pages = {361–416},
numpages = {56}
}

@inproceedings{10.1145/3121257.3121262,
author = {Czech, Mike and H\"{u}llermeier, Eyke and Jakobs, Marie-Christine and Wehrheim, Heike},
title = {Predicting Rankings of Software Verification Tools},
year = {2017},
isbn = {9781450351577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121257.3121262},
doi = {10.1145/3121257.3121262},
abstract = {Today, software verification tools have reached the maturity to be used for large scale programs. Different tools perform differently well on varying code. A software developer is hence faced with the problem of choosing a tool appropriate for her program at hand. A ranking of tools on programs could facilitate the choice. Such rankings can, however, so far only be obtained by running all considered tools on the program. In this paper, we present a machine learning approach to predicting rankings of tools on programs. The method builds upon so-called label ranking algorithms, which we complement with appropriate kernels providing a similarity measure for programs. Our kernels employ a graph representation for software source code that mixes elements of control flow and program dependence graphs with abstract syntax trees. Using data sets from the software verification competition SV-COMP, we demonstrate our rank prediction technique to generalize well and achieve a rather high predictive accuracy (rank correlation > 0.6).},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Analytics},
pages = {23–26},
numpages = {4},
keywords = {ranking, machine learning, Software verification},
location = {Paderborn, Germany},
series = {SWAN 2017}
}

@InProceedings{blicha2022transition,
author="Blicha, Martin
and Fedyukovich, Grigory
and Hyv{\"a}rinen, Antti E. J.
and Sharygina, Natasha",
editor="Fisman, Dana
and Rosu, Grigore",
title="Transition Power Abstractions for Deep Counterexample Detection",
booktitle="Tools and Algorithms for the Construction and Analysis of Systems",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="524--542",
abstract="While model checking safety of infinite-state systems by inferring state invariants has steadily improved recently, most verification tools still rely on a technique based on bounded model checking to detect safety violations. In particular, the current techniques typically analyze executions by unfolding transitions one step at a time, and the slow growth of execution length prevents detection of deep counterexamples before the tool reaches its limits on computations. We propose a novel model-checking algorithm that is capable of both proving unbounded safety and finding long counterexamples. The idea is to use Craig interpolation to guide the creation of symbolic abstractions of exponentially longer sequences of transitions. Our experimental analysis shows that on unsafe benchmarks with deep counterexamples our implementation can detect faulty executions that are at least an order of magnitude longer than those detectable by the state-of-the-art tools.",
isbn="978-3-030-99524-9"
}

@inproceedings{10.1145/512950.512973,
author = {Cousot, Patrick and Cousot, Radhia},
title = {Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints},
year = {1977},
isbn = {9781450373500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512950.512973},
doi = {10.1145/512950.512973},
abstract = {A program denotes computations in some universe of objects. Abstract interpretation of programs consists in using that denotation to describe computations in another universe of abstract objects, so that the results of abstract execution give some information on the actual computations. An intuitive example (which we borrow from Sintzoff [72]) is the rule of signs. The text -1515 * 17 may be understood to denote computations on the abstract universe {(+), (-), (±)} where the semantics of arithmetic operators is defined by the rule of signs. The abstract execution -1515 * 17 → -(+) * (+) → (-) * (+) → (-), proves that -1515 * 17 is a negative number. Abstract interpretation is concerned by a particular underlying structure of the usual universe of computations (the sign, in our example). It gives a summary of some facets of the actual executions of a program. In general this summary is simple to obtain but inaccurate (e.g. -1515 + 17 → -(+) + (+) → (-) + (+) → (±)). Despite its fundamentally incomplete results abstract interpretation allows the programmer or the compiler to answer questions which do not need full knowledge of program executions or which tolerate an imprecise answer, (e.g. partial correctness proofs of programs ignoring the termination problems, type checking, program optimizations which are not carried in the absence of certainty about their feasibility, …).},
booktitle = {Proceedings of the 4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
pages = {238–252},
numpages = {15},
location = {Los Angeles, California},
series = {POPL '77}
}

@article{10.1145/3498721,
author = {Campion, Marco and Dalla Preda, Mila and Giacobazzi, Roberto},
title = {Partial (In)Completeness in Abstract Interpretation: Limiting the Imprecision in Program Analysis},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {POPL},
url = {https://doi.org/10.1145/3498721},
doi = {10.1145/3498721},
abstract = {Imprecision is inherent in any decidable (sound) approximation of undecidable program properties. In abstract interpretation this corresponds to the release of false alarms, e.g., when it is used for program analysis and program verification. As all alarming systems, a program analysis tool is credible when few false alarms are reported. As a consequence, we have to live together with false alarms, but also we need methods to control them. As for all approximation methods, also for abstract interpretation we need to estimate the accumulated imprecision during program analysis. In this paper we introduce a theory for estimating the error propagation in abstract interpretation, and hence in program analysis. We enrich abstract domains with a weakening of a metric distance. This enriched structure keeps coherence between the standard partial order relating approximated objects by their relative precision and the effective error made in this approximation. An abstract interpretation is precise when it is complete. We introduce the notion of partial completeness as a weakening of precision. In partial completeness the abstract interpreter may produce a bounded number of false alarms. We prove the key recursive properties of the class of programs for which an abstract interpreter is partially complete with a given bound of imprecision. Then, we introduce a proof system for estimating an upper bound of the error accumulated by the abstract interpreter during program analysis. Our framework is general enough to be instantiated to most known metrics for abstract domains.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {59},
numpages = {31},
keywords = {Program Analysis, Abstract Domain, Abstract Interpretation, Partial Completeness}
}

@article{10.1145/371282.371285,
author = {Blass, Andreas and Gurevich, Yuri},
title = {Inadequacy of Computable Loop Invariants},
year = {2001},
issue_date = {Jan. 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1529-3785},
url = {https://doi.org/10.1145/371282.371285},
doi = {10.1145/371282.371285},
abstract = {Hoare logic is a widely recommended verification tool. There is, however, a problem of finding easily checkable loop invariants; it is known that decidable assertions do not suffice to verify while programs, even when the pre- and postconditions are decidable. We show here a stronger result: decidable invariants do not suffice to verify single-loop programs. We also show that this problem arises even in extremely simple contexts. Let N be the structure consisting of the set of natural numbers together with the functions S(x)=x+1,D(x)=2(x)=***x/2***. There is a single-loop program *** using only three variables x,y,z such that the asserted program x=y=z=0 *** false is partially correct on N but any loop invariant I(x,y,z) for this asserted program is undecidable.},
journal = {ACM Trans. Comput. Logic},
month = {jan},
pages = {1–11},
numpages = {11},
keywords = {automated deduction, loop invariants, assertion, Hoare logic, recursive inseparability, precondition, automated reasoning, postcondition uncomputable}
}

@Inbook{Bjorner2015,
author="Bj{\o}rner, Nikolaj
and Gurfinkel, Arie
and McMillan, Ken
and Rybalchenko, Andrey",
editor="Beklemishev, Lev D.
and Blass, Andreas
and Dershowitz, Nachum
and Finkbeiner, Bernd
and Schulte, Wolfram",
title="Horn Clause Solvers for Program Verification",
bookTitle="Fields of Logic and Computation II: Essays Dedicated to Yuri Gurevich on the Occasion of His 75th Birthday",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="24--51",
abstract="Automatic program verification and symbolic model checking tools interface with theorem proving technologies that check satisfiability of formulas. A theme pursued in the past years by the authors of this paper has been to encode symbolic model problems directly as Horn clauses and develop dedicated solvers for Horn clauses. Our solvers are called Duality, HSF, SeaHorn, and {\$}{\$}{\backslash}mu {\{}Z{\}}{\$}{\$}and we have devoted considerable attention in recent papers to algorithms for solving Horn clauses. This paper complements these strides as we summarize main useful properties of Horn clauses, illustrate encodings of procedural program verification into Horn clauses and then highlight a number of useful simplification strategies at the level of Horn clauses. Solving Horn clauses amounts to establishing Existential positive Fixed-point Logic formulas, a perspective that was promoted by Blass and Gurevich.",
isbn="978-3-319-23534-9",
doi="10.1007/978-3-319-23534-9_2",
url="https://doi.org/10.1007/978-3-319-23534-9_2"
}

@article{10.1145/3022187,
author = {Karbyshev, Aleksandr and Bj\o{}rner, Nikolaj and Itzhaky, Shachar and Rinetzky, Noam and Shoham, Sharon},
title = {Property-Directed Inference of Universal Invariants or Proving Their Absence},
year = {2017},
issue_date = {February 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {1},
issn = {0004-5411},
url = {https://doi.org/10.1145/3022187},
doi = {10.1145/3022187},
abstract = {We present Universal Property Directed Reachability (PDR∀), a property-directed semi-algorithm for automatic inference of invariants in a universal fragment of first-order logic. PDR∀ is an extension of Bradley’s PDR/IC3 algorithm for inference of propositional invariants. PDR∀ terminates when it discovers a concrete counterexample, infers an inductive universal invariant strong enough to establish the desired safety property, or finds a proof that such an invariant does not exist. PDR∀ is not guaranteed to terminate. However, we prove that under certain conditions, for example, when reasoning about programs manipulating singly linked lists, it does.We implemented an analyzer based on PDR∀ and applied it to a collection of list-manipulating programs. Our analyzer was able to automatically infer universal invariants strong enough to establish memory safety and certain functional correctness properties, show the absence of such invariants for certain natural programs and specifications, and detect bugs. All this without the need for user-supplied abstraction predicates.},
journal = {J. ACM},
month = {mar},
articleno = {7},
numpages = {33},
keywords = {Universal invariants, EPR, PDR, property-directed reachability, IC3}
}

@misc{kossak2023undefinability,
      title={Undefinability and Absolute Undefinability in Arithmetic}, 
      author={Roman Kossak},
      year={2023},
      eprint={2205.06022},
      archivePrefix={arXiv},
      primaryClass={math.LO}
}

@article{comon1994equational,
title = {Equational Formulas with Membership Constraints},
journal = {Information and Computation},
volume = {112},
number = {2},
pages = {167-216},
year = {1994},
issn = {0890-5401},
doi = {https://doi.org/10.1006/inco.1994.1056},
url = {https://www.sciencedirect.com/science/article/pii/S089054018471056X},
author = {H. Comon and C. Delor},
abstract = {We propose a set of transformation rules for first order formulae whose atoms are either equations between terms or "membership constraints" t∈ζ. ζ can be interpreted as a regular tree language (ζ is called a sort in the algebraic specification community) or as a tree language in any class of languages which satisfies some adequate closure and decidability properties. This set of rules is proved to be correct, terminating, and complete. This provides a quantifier elimination procedure: for every regular tree language L, the first order theory of some structure defining L is decidable. This extends the results of several previously published results. We also show how to apply our results to automatic inductive proofs in equational theories.}
}

@InProceedings{zhang2004decision,
author="Zhang, Ting
and Sipma, Henny B.
and Manna, Zohar",
editor="Basin, David
and Rusinowitch, Micha{\"e}l",
title="Decision Procedures for Recursive Data Structures with Integer Constraints",
booktitle="Automated Reasoning",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="152--167",
abstract="This paper is concerned with the integration of recursive data structures with Presburger arithmetic. The integrated theory includes a length function on data structures, thus providing a tight coupling between the two theories, and hence the general Nelson-Oppen combination method for decision procedures is not applicable to this theory, even for the quantifier-free case. We present four decision procedures for the integrated theory depending on whether the language has infinitely many constants and whether the theory has quantifiers. Our decision procedures for quantifier-free theories are based on Oppen's algorithm for acyclic recursive data structures with infinite atom domain.",
isbn="978-3-540-25984-8"
}

@INPROCEEDINGS{hojjat2017deciding,
  author={Hojjat, Hossein and Rümmer, Philipp},
  booktitle={2017 19th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)}, 
  title={Deciding and Interpolating Algebraic Data Types by Reduction}, 
  year={2017},
  volume={},
  number={},
  pages={145-152},
  doi={10.1109/SYNASC.2017.00033}}

@inproceedings{oppen1980reasoning,
author = {Oppen, Derek C.},
title = {Reasoning about Recursively Defined Data Structures},
year = {1978},
isbn = {9781450373487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/512760.512776},
doi = {10.1145/512760.512776},
abstract = {A decision algorithm is given for the quantifier-free theory of recursively defined data structures which, for a conjunction of length n, decides its satisfiability in time linear in n. The first-order theory of recursively defined data structures, in particular the first-order theory of LISP list structure (the theory of CONS, CAR, CDR), is shown to be decidable but not elementary recursive.},
booktitle = {Proceedings of the 5th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
pages = {151–157},
numpages = {7},
location = {Tucson, Arizona},
series = {POPL '78}
}

@book{tata,
  TITLE = {{Tree Automata Techniques and Applications}},
  AUTHOR = {Comon, Hubert and Dauchet, Max and Gilleron, R{\'e}mi and Jacquemard, Florent and Lugiez, Denis and L{\"o}ding, Christof and Tison, Sophie and Tommasi, Marc},
  URL = {https://hal.inria.fr/hal-03367725},
  PAGES = {262},
  YEAR = {2008},
  PDF = {https://hal.inria.fr/hal-03367725/file/tata.pdf},
  HAL_ID = {hal-03367725},
  HAL_VERSION = {v1},
}

@article{matzinger1998computational,
  title={On computational representations of Herbrand models},
  author={Matzinger, Robert},
  journal={Uwe Egly and Hans Tompits, editors},
  volume={13},
  pages={86--95},
  year={1998}
}

@phdthesis{matzinger2000computational,
  title={Computational representations of models in first-order logic},
  author={Matzinger, Robert},
  year={2000},
  school={Technische Universität Wien, Austria}
}


@inproceedings{chabin2007visibly,
  title={Visibly pushdown languages and term rewriting},
  author={Chabin, Jacques and R{\'e}ty, Pierre},
  booktitle={International Symposium on Frontiers of Combining Systems},
  pages={252--266},
  year={2007},
  organization={Springer}
}
@inproceedings{gouranton2001synchronized,
  title={Synchronized tree languages revisited and new applications},
  author={Gouranton, Val{\'e}rie and R{\'e}ty, Pierre and Seidl, Helmut},
  booktitle={International Conference on Foundations of Software Science and Computation Structures},
  pages={214--229},
  year={2001},
  organization={Springer}
}
@inproceedings{limet2001weakly,
  title={Weakly regular relations and applications},
  author={Limet, S{\'e}bastien and R{\'e}ty, Pierre and Seidl, Helmut},
  booktitle={International Conference on Rewriting Techniques and Applications},
  pages={185--200},
  year={2001},
  organization={Springer}
}
@techreport{chabin2006synchronized,
  title={Synchronized-context free tree-tuple languages},
  author={Chabin, Jacques and Chen, Jing and R{\'e}ty, Pierre},
  year={2006},
  institution={Citeseer}
}
@inproceedings{jacquemard2009rigid,
  title={Rigid tree automata},
  author={Jacquemard, Florent and Klay, Francis and Vacher, Camille},
  booktitle={International Conference on Language and Automata Theory and Applications},
  pages={446--457},
  year={2009},
  organization={Springer}
}
@inproceedings{engelfriet2017multiple,
  title={Multiple context-free tree grammars and multi-component tree adjoining grammars},
  author={Engelfriet, Joost and Maletti, Andreas},
  booktitle={International Symposium on Fundamentals of Computation Theory},
  pages={217--229},
  year={2017},
  organization={Springer}
}

@PHDTHESIS{haude2020,
url = "http://www.theses.fr/2020REN1S060",
title = "Automatic verification of higher-order functional programs using regular tree languages",
author = "Haudebourg, Timothée",
year = "2020",
note = "Thèse de doctorat dirigée par Genet, Thomas et Jensen, Thomas Informatique Rennes 1 2020",
note = "2020REN1S060",
}

@article{angelis_fioravanti_pettorossi_proietti_2018, title={Solving Horn Clauses on Inductive Data Types Without Induction}, volume={18}, DOI={10.1017/S1471068418000157}, number={3-4}, journal={Theory and Practice of Logic Programming}, publisher={Cambridge University Press}, author={De Angelis, Emanuele and Fioravanti, Fabio and Pettorossi, Alberto and Proietti, Maurizio}, year={2018}, pages={452–469}}

@inproceedings{blass2000the,
author = {Blass, Andreas and Gurevich, Yuri},
title = {The Underlying Logic of Hoare Logic},
booktitle = {Bulletin of the European Association for Theoretical Computer Science},
year = {2000},
month = {February},
abstract = {Formulas of Hoare logic are asserted programs φ P ψ where P is a program and φ, ψ are assertions. The language of programs varies; in the 1980 survey of K. Apt, one finds the language of while programs and various extensions of it. But the assertions are traditionally expressed in first-order logic (or extensions of it). In that sense, first-order logic is the underlying logic of Hoare logic. We question the tradition and demonstrate, on the simple example of while programs, that alternative assertion logics have some advantages. For some natural assertion logics, the expressivity hypothesis in Cook's completeness theorem is automatically satisfied.},
url = {https://www.microsoft.com/en-us/research/publication/142-underlying-logic-hoare-logic/},
pages = {82-110},
volume = {70},
edition = {Bulletin of the European Association for Theoretical Computer Science},
}

@inproceedings{10.1145/2254064.2254112,
author = {Grebenshchikov, Sergey and Lopes, Nuno P. and Popeea, Corneliu and Rybalchenko, Andrey},
title = {Synthesizing Software Verifiers from Proof Rules},
year = {2012},
isbn = {9781450312059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254064.2254112},
doi = {10.1145/2254064.2254112},
abstract = {Automatically generated tools can significantly improve programmer productivity. For example, parsers and dataflow analyzers can be automatically generated from declarative specifications in the form of grammars, which tremendously simplifies the task of implementing a compiler. In this paper, we present a method for the automatic synthesis of software verification tools. Our synthesis procedure takes as input a description of the employed proof rule, e.g., program safety checking via inductive invariants, and produces a tool that automatically discovers the auxiliary assertions required by the proof rule, e.g., inductive loop invariants and procedure summaries. We rely on a (standard) representation of proof rules using recursive equations over the auxiliary assertions. The discovery of auxiliary assertions, i.e., solving the equations, is based on an iterative process that extrapolates solutions obtained for finitary unrollings of equations. We show how our method synthesizes automatic safety and liveness verifiers for programs with procedures, multi-threaded programs, and functional programs. Our experimental comparison of the resulting verifiers with existing state-of-the-art verification tools confirms the practicality of the approach.},
booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {405–416},
numpages = {12},
keywords = {software model checking, software verification, proof rules, verification tool synthesis},
location = {Beijing, China},
series = {PLDI '12}
}

@INPROCEEDINGS{8327565,
  author={Wohrer, Maximilian and Zdun, Uwe},
  booktitle={2018 International Workshop on Blockchain Oriented Software Engineering (IWBOSE)}, 
  title={Smart contracts: security patterns in the ethereum ecosystem and solidity}, 
  year={2018},
  volume={},
  number={},
  pages={2-8},
  doi={10.1109/IWBOSE.2018.8327565}}

@InProceedings{10.1007/978-3-642-37036-6_8,
author="Filli{\^a}tre, Jean-Christophe
and Paskevich, Andrei",
editor="Felleisen, Matthias
and Gardner, Philippa",
title="Why3 --- Where Programs Meet Provers",
booktitle="Programming Languages and Systems",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="125--128",
abstract="We present Why3, a tool for deductive program verification, and WhyML, its programming and specification language. WhyML is a first-order language with polymorphic types, pattern matching, and inductive predicates. Programs can make use of record types with mutable fields, type invariants, and ghost code. Verification conditions are discharged by Why3 with the help of various existing automated and interactive theorem provers. To keep verification conditions tractable and comprehensible, WhyML imposes a static control of aliases that obviates the use of a memory model. A user can write WhyML programs directly and get correct-by-construction OCaml programs via an automated extraction mechanism. WhyML is also used as an intermediate language for the verification of C, Java, or Ada programs. We demonstrate the benefits of Why3 and WhyML on non-trivial examples of program verification.",
isbn="978-3-642-37036-6"
}

@Inbook{Blass1987,
author="Blass, Andreas
and Gurevich, Yuri",
editor="B{\"o}rger, Egon",
title="Existential fixed-point logic",
bookTitle="Computation Theory and Logic",
year="1987",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="20--36",
abstract="The purpose of this paper is to draw attention to existential fixed-point logic. Among other things, we show that: (1) If a structure A satisfies an existential fixed-point formula $\phi$, then A has a finite subset F such that every structure B with A|F = B|F satisfies $\phi$. (2) Using existential fixed-point logic instead of first-order logic removes the expressivity hypothesis in Cook's completeness theorem for Hoare logic. (3) In the presence of a successor relation, existential fixed-point logic captures polynomial time.",
isbn="978-3-540-47795-2",
doi="10.1007/3-540-18170-9_151",
url="https://doi.org/10.1007/3-540-18170-9_151"
}

@inproceedings{Floyd1993,
Author = {Robert W. Floyd},
Booktitle = {Proccedings of the AMS Symposium on Appllied Mathematics},
Pages = {19-31},
Publisher = {American Mathematical Society},
Title = {Assigning meanings to programms},
Volume = {19},
Year = {1967}}

@article{doi:10.1137/0207005,
author = {Cook, Stephen A.},
title = {Soundness and Completeness of an Axiom System for Program Verification},
journal = {SIAM Journal on Computing},
volume = {7},
number = {1},
pages = {70-90},
year = {1978},
doi = {10.1137/0207005},
URL = { 
        https://doi.org/10.1137/0207005
},
eprint = {
        https://doi.org/10.1137/0207005
}
,
    abstract = { A simple ALGOL-like language is defined which includes conditional, while, and procedure call statements as well as blocks. A formal interpretive semantics and a Hoare style axiom system are given for the language. The axiom system is proved to be sound, and in a certain sense complete, relative to the interpretive semantics. The main new results are the completeness theorem, and a careful treatment of the procedure call rules for procedures with global variables in their declarations. }
}

@inproceedings{10.1145/292540.292560,
author = {Xi, Hongwei and Pfenning, Frank},
title = {Dependent Types in Practical Programming},
year = {1999},
isbn = {1581130953},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/292540.292560},
doi = {10.1145/292540.292560},
abstract = {We present an approach to enriching the type system of ML with a restricted form of dependent types, where type index objects are drawn from a constraint domain C, leading to the DML(C) language schema. This allows specification and inference of significantly more precise type information, facilitating program error detection and compiler optimization. A major complication resulting from introducing dependent types is that pure type inference for the enriched system is no longer possible, but we show that type-checking a sufficiently annotated program in DML(C) can be reduced to constraint satisfaction in the constraint domain C. We exhibit the unobtrusiveness of our approach through practical examples and prove that DML(C) is conservative over ML. The main contribution of the paper lies in our language design, including the formulation of type-checking rules which makes the approach practical. To our knowledge, no previous type system for a general purpose programming language such as ML has combined dependent types with features including datatype declarations, higher-order functions, general recursions, let-polymorphism, mutable references, and exceptions. In addition, we have finished a prototype implementation of DML(C) for an integer constraint domain C, where constraints are linear inequalities (Xi and Pfenning 1998).},
booktitle = {Proceedings of the 26th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {214–227},
numpages = {14},
location = {San Antonio, Texas, USA},
series = {POPL '99}
}

@ARTICLE{713327,
  author={Rushby, J. and Owre, S. and Shankar, N.},
  journal={IEEE Transactions on Software Engineering}, 
  title={Subtypes for specifications: predicate subtyping in PVS}, 
  year={1998},
  volume={24},
  number={9},
  pages={709-720},
  doi={10.1109/32.713327}
}

@inproceedings{10.1145/2837614.2837655,
author = {Swamy, Nikhil and Hri\c{t}cu, C\u{a}t\u{a}lin and Keller, Chantal and Rastogi, Aseem and Delignat-Lavaud, Antoine and Forest, Simon and Bhargavan, Karthikeyan and Fournet, C\'{e}dric and Strub, Pierre-Yves and Kohlweiss, Markulf and Zinzindohoue, Jean-Karim and Zanella-B\'{e}guelin, Santiago},
title = {Dependent Types and Multi-Monadic Effects in F*},
year = {2016},
isbn = {9781450335492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837614.2837655},
doi = {10.1145/2837614.2837655},
abstract = {We present a new, completely redesigned, version of F*, a language that works both as a proof assistant as well as a general-purpose, verification-oriented, effectful programming language. In support of these complementary roles, F* is a dependently typed, higher-order, call-by-value language with _primitive_ effects including state, exceptions, divergence and IO. Although primitive, programmers choose the granularity at which to specify effects by equipping each effect with a monadic, predicate transformer semantics. F* uses this to efficiently compute weakest preconditions and discharges the resulting proof obligations using a combination of SMT solving and manual proofs. Isolated from the effects, the core of F* is a language of pure functions used to write specifications and proof terms---its consistency is maintained by a semantic termination check based on a well-founded order. We evaluate our design on more than 55,000 lines of F* we have authored in the last year, focusing on three main case studies. Showcasing its use as a general-purpose programming language, F* is programmed (but not verified) in F*, and bootstraps in both OCaml and F#. Our experience confirms F*'s pay-as-you-go cost model: writing idiomatic ML-like code with no finer specifications imposes no user burden. As a verification-oriented language, our most significant evaluation of F* is in verifying several key modules in an implementation of the TLS-1.2 protocol standard. For the modules we considered, we are able to prove more properties, with fewer annotations using F* than in a prior verified implementation of TLS-1.2. Finally, as a proof assistant, we discuss our use of F* in mechanizing the metatheory of a range of lambda calculi, starting from the simply typed lambda calculus to System F-omega and even micro-F*, a sizeable fragment of F* itself---these proofs make essential use of F*'s flexible combination of SMT automation and constructive proofs, enabling a tactic-free style of programming and proving at a relatively large scale.},
booktitle = {Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {256–270},
numpages = {15},
keywords = {proof assistants, verification, effectful programming},
location = {St. Petersburg, FL, USA},
series = {POPL '16}
}
@misc{https://doi.org/10.48550/arxiv.2207.04034,
  doi = {10.48550/ARXIV.2207.04034},
  
  url = {https://arxiv.org/abs/2207.04034},
  
  author = {Lehmann, Nico and Geller, Adam and Vazou, Niki and Jhala, Ranjit},
  
  keywords = {Programming Languages (cs.PL), FOS: Computer and information sciences, FOS: Computer and information sciences, F.3.1; D.2.4},
  
  title = {Flux: Liquid Types for Rust},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@book{ClocksinMellish03,
  abstract = {Originally published in 1981, this was the first textbook on programming in the Prolog language and is still the definitive introductory text on Prolog. Though many Prolog textbooks have been published since, this one has withstood the test of time because of its comprehensiveness, tutorial approach, and emphasis on general programming applications. Prolog has continued to attract a great deal of interest in the computer science community, and has turned out to be a basis for an important new generation of programming languages and systems for Artificial Intelligence. Since the previous edition of Programming in Prolog, the language has been standardised by the International Organization for Standardization (ISO) and this book has been updated accordingly. The authors have also introduced some new material, clarified some explanations, corrected a number of minor errors, and removed appendices about Prolog systems that are now obsolete.},
  added-at = {2016-08-15T08:28:57.000+0200},
  address = {Berlin},
  author = {Clocksin, William F. and Mellish, Christopher S.},
  biburl = {https://www.bibsonomy.org/bibtex/2f7d8c9d8fc4ff10019644968a955a2a5/flint63},
  description = {Edition 3 1987 ISBN 978-0-387-17539-3},
  doi = {10.1007/978-3-642-55481-0},
  edition = 5,
  file = {SpringerLink:2000-04/ClocksinMellish03.pdf:PDF;Springer Product page:http\://www.springer.com/978-3-540-00678-7:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/3540006788/:URL},
  groups = {public},
  interhash = {c634da1a1269623500f0223aa4237b47},
  intrahash = {f7d8c9d8fc4ff10019644968a955a2a5},
  isbn = {978-3-540-00678-7},
  keywords = {01624 103 springer book ai software development logic prolog},
  publisher = {Springer},
  timestamp = {2017-07-13T17:17:27.000+0200},
  title = {Programming in Prolog},
  username = {flint63},
  year = 2003
}

@InProceedings{10.1007/978-3-030-30048-7_35,
author="Yang, Weikun
and Fedyukovich, Grigory
and Gupta, Aarti",
editor="Schiex, Thomas
and de Givry, Simon",
title="Lemma Synthesis for Automating Induction over Algebraic Data Types",
booktitle="Principles and Practice of Constraint Programming",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="600--617",
abstract="In this paper we introduce a new approach for proving quantified theorems over inductively defined data-types. We present an automated prover that searches for a sequence of simplifications and transformations to prove the validity of a given theorem, and in the absence of required lemmas, attempts to synthesize supporting lemmas based on terms and expressions witnessed during the search for a proof. The search for lemma candidates is guided by a user-specified template, along with many automated filtering mechanisms. Validity of generated lemmas is checked recursively by our prover, supported by an off-the-shelf SMT solver. We have implemented our prover called AdtInd and show that it is able to solve many problems on which a state-of-the-art prover fails.",
isbn="978-3-030-30048-7"
}

@InProceedings{reynolds2015induction,
author="Reynolds, Andrew
and Kuncak, Viktor",
editor="D'Souza, Deepak
and Lal, Akash
and Larsen, Kim Guldstrand",
title="Induction for SMT Solvers",
booktitle="Verification, Model Checking, and Abstract Interpretation",
year="2015",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="80--98",
abstract="Satisfiability modulo theory solvers are increasingly being used to solve quantified formulas over structures such as integers and term algebras. Quantifier instantiation combined with ground decision procedure alone is insufficient to prove many formulas of interest in such cases. We present a set of techniques that introduce inductive reasoning into SMT solving algorithms that is sound with respect to the interpretation of structures in SMT-LIB standard. The techniques include inductive strengthening of conjecture to be proven, as well as facility to automatically discover subgoals during an inductive proof, where subgoals themselves can be proven using induction. The techniques have been implemented in CVC4. Our experiments show that the developed techniques have good performance and coverage of a range of inductive reasoning problems. Our experiments also show the impact of different representations of natural numbers and quantifier instantiation techniques on the performance of inductive reasoning. Our solution is freely available in the CVC4 development repository. In addition its overall effectiveness, it has an advantage of accepting SMT-LIB input and being integrated with other SMT solving techniques of CVC4.",
isbn="978-3-662-46081-8"
}

@InProceedings{10.1007/978-3-319-63390-9_30,
author="Unno, Hiroshi
and Torii, Sho
and Sakamoto, Hiroki",
editor="Majumdar, Rupak
and Kun{\v{c}}ak, Viktor",
title="Automating Induction for Solving Horn Clauses",
booktitle="Computer Aided Verification",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="571--591",
abstract="Verification problems of programs in various paradigms can be reduced to problems of solving Horn clause constraints on predicate variables that represent unknown inductive invariants. This paper presents a novel Horn constraint solving method based on inductive theorem proving: the method reduces Horn constraint solving to validity checking of first-order formulas with inductively defined predicates, which are then checked by induction on the derivation of the predicates. To automate inductive proofs, we introduce a novel proof system tailored to Horn constraint solving, and use a PDR-based Horn constraint solver as well as an SMT solver to discharge proof obligations arising in the proof search. We prove that our proof system satisfies the soundness and relative completeness with respect to ordinary Horn constraint solving schemes. The two main advantages of the proposed method are that (1) it can deal with constraints over any background theories supported by the underlying SMT solver, including nonlinear arithmetic and algebraic data structures, and (2) the method can verify relational specifications across programs in various paradigms where multiple function calls need to be analyzed simultaneously. The class of specifications includes practically important ones such as functional equivalence, associativity, commutativity, distributivity, monotonicity, idempotency, and non-interference. Our novel combination of Horn clause constraints with inductive theorem proving enables us to naturally and automatically axiomatize recursive functions that are possibly non-terminating, non-deterministic, higher-order, exception-raising, and over non-inductively defined data types. We have implemented a relational verification tool for the OCaml functional language based on the proposed method and obtained promising results in preliminary experiments.",
isbn="978-3-319-63390-9"
}

@article{10.1145/3360592,
author = {Hamza, Jad and Voirol, Nicolas and Kun\v{c}ak, Viktor},
title = {System FR: Formalized Foundations for the Stainless Verifier},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360592},
doi = {10.1145/3360592},
abstract = {We present the design, implementation, and foundation of a verifier for higher-order functional programs with generics and recursive data types. Our system supports proving safety and termination using preconditions, postconditions and assertions. It supports writing proof hints using assertions and recursive calls. To formalize the soundness of the system we introduce System FR, a calculus supporting System F polymorphism, dependent refinement types, and recursive types (including recursion through contravariant positions of function types). Through the use of sized types, System FR supports reasoning about termination of lazy data structures such as streams. We formalize a reducibility argument using the Coq proof assistant and prove the soundness of a type-checker with respect to call-by-value semantics, ensuring type safety and normalization for typeable programs. Our program verifier is implemented as an alternative verification-condition generator for the Stainless tool, which relies on the Inox SMT-based solver backend for automation. We demonstrate the efficiency of our approach by verifying a collection of higher-order functional programs comprising around 14000 lines of polymorphic higher-order Scala code, including graph search algorithms, basic number theory, monad laws, functional data structures, and assignments from popular Functional Programming MOOCs.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {166},
numpages = {30},
keywords = {software verification, dependent types, recursive types, System F, SMT}
}


@InProceedings{10.1007/978-3-642-23702-7_23,
author="Suter, Philippe
and K{\"o}ksal, Ali Sinan
and Kuncak, Viktor",
editor="Yahav, Eran",
title="Satisfiability Modulo Recursive Programs",
booktitle="Static Analysis",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="298--315",
abstract="We present a semi-decision procedure for checking satisfiability of expressive correctness properties of recursive first-order functional programs. In our approach, both properties and programs are expressed in the same language, a subset of Scala. We implemented our procedure and integrated it with the Z3 SMT solver and the Scala compiler. Our procedure is sound for counterexamples and for proofs of terminating functions. It is terminating and thus complete for many important classes of specifications, including all satisfiable formulas and all formulas where recursive functions satisfy certain syntactic restrictions. Using our system, Leon, we verified detailed correctness properties for functional data structure implementations, as well as syntax tree manipulations. We have found our system to be fast for both finding counterexamples and finding correctness proofs, and to scale to larger programs than alternative techniques.",
isbn="978-3-642-23702-7"
}



@article{10.1145/2914770.2837655,
author = {Swamy, Nikhil and Hri\c{t}cu, C\u{a}t\u{a}lin and Keller, Chantal and Rastogi, Aseem and Delignat-Lavaud, Antoine and Forest, Simon and Bhargavan, Karthikeyan and Fournet, C\'{e}dric and Strub, Pierre-Yves and Kohlweiss, Markulf and Zinzindohoue, Jean-Karim and Zanella-B\'{e}guelin, Santiago},
title = {Dependent Types and Multi-Monadic Effects in F*},
year = {2016},
issue_date = {January 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/2914770.2837655},
doi = {10.1145/2914770.2837655},
abstract = {We present a new, completely redesigned, version of F*, a language that works both as a proof assistant as well as a general-purpose, verification-oriented, effectful programming language. In support of these complementary roles, F* is a dependently typed, higher-order, call-by-value language with _primitive_ effects including state, exceptions, divergence and IO. Although primitive, programmers choose the granularity at which to specify effects by equipping each effect with a monadic, predicate transformer semantics. F* uses this to efficiently compute weakest preconditions and discharges the resulting proof obligations using a combination of SMT solving and manual proofs. Isolated from the effects, the core of F* is a language of pure functions used to write specifications and proof terms---its consistency is maintained by a semantic termination check based on a well-founded order. We evaluate our design on more than 55,000 lines of F* we have authored in the last year, focusing on three main case studies. Showcasing its use as a general-purpose programming language, F* is programmed (but not verified) in F*, and bootstraps in both OCaml and F#. Our experience confirms F*'s pay-as-you-go cost model: writing idiomatic ML-like code with no finer specifications imposes no user burden. As a verification-oriented language, our most significant evaluation of F* is in verifying several key modules in an implementation of the TLS-1.2 protocol standard. For the modules we considered, we are able to prove more properties, with fewer annotations using F* than in a prior verified implementation of TLS-1.2. Finally, as a proof assistant, we discuss our use of F* in mechanizing the metatheory of a range of lambda calculi, starting from the simply typed lambda calculus to System F-omega and even micro-F*, a sizeable fragment of F* itself---these proofs make essential use of F*'s flexible combination of SMT automation and constructive proofs, enabling a tactic-free style of programming and proving at a relatively large scale.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {256–270},
numpages = {15},
keywords = {proof assistants, effectful programming, verification}
}

@article{10.1145/2692915.2628161,
author = {Vazou, Niki and Seidel, Eric L. and Jhala, Ranjit and Vytiniotis, Dimitrios and Peyton-Jones, Simon},
title = {Refinement Types for Haskell},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/2692915.2628161},
doi = {10.1145/2692915.2628161},
abstract = {SMT-based checking of refinement types for call-by-value languages is a well-studied subject. Unfortunately, the classical translation of refinement types to verification conditions is unsound under lazy evaluation. When checking an expression, such systems implicitly assume that all the free variables in the expression are bound to values. This property is trivially guaranteed by eager, but does not hold under lazy, evaluation. Thus, to be sound and precise, a refinement type system for Haskell and the corresponding verification conditions must take into account which subset of binders actually reduces to values. We present a stratified type system that labels binders as potentially diverging or not, and that (circularly) uses refinement types to verify the labeling. We have implemented our system in LIQUIDHASKELL and present an experimental evaluation of our approach on more than 10,000 lines of widely used Haskell libraries. We show that LIQUIDHASKELL is able to prove 96% of all recursive functions terminating, while requiring a modest 1.7 lines of termination-annotations per 100 lines of code.},
journal = {SIGPLAN Not.},
month = {aug},
pages = {269–282},
numpages = {14}
}

@inproceedings{10.1145/1900160.1900170,
author = {Eriksen, Marius},
title = {Scaling Scala at Twitter},
year = {2010},
isbn = {9781450305167},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1900160.1900170},
doi = {10.1145/1900160.1900170},
abstract = {Rockdove is the backend service that powers the geospatial features on Twitter.com and the Twitter API ("Twitter Places"). It provides a datastore for places and a geospatial search engine to find them. To throw out some buzzwords, it is:•a distributed system•realtime (immediately indexes updates and changes)•horizontally scalable•fault tolerantRockdove is written entirely in Scala and was developed by 2 engineers with no prior Scala experience (nor with Java or the JVM). We think the geospatial search engine provides an interesting case study as it presents a mix of algorithm problems and "classic" scaling and optimization issues. We will report on our experience using Scala, focusing especially on:•"functional" systems design•concurrency and parallelism•using a "research language" in practice•when, where and why we turned the "functional dial"•avoiding mutable state},
booktitle = {ACM SIGPLAN Commercial Users of Functional Programming},
articleno = {8},
numpages = {1},
location = {Baltimore, Maryland},
series = {CUFP '10}
}
